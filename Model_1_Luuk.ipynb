{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThe goal of this competition is to **detect freezing of gait (FOG)**, a debilitating symptom that afflicts many people **with Parkinson’s disease**. It is requred to **develop a machine learning model trained on data collected from a wearable 3D lower back sensor** to better understand **when and why FOG episodes occur**.","metadata":{"papermill":{"duration":0.026599,"end_time":"2023-05-14T06:28:35.319714","exception":false,"start_time":"2023-05-14T06:28:35.293115","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{"papermill":{"duration":0.024773,"end_time":"2023-05-14T06:28:35.368114","exception":false,"start_time":"2023-05-14T06:28:35.343341","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!ls ../input/tsflex/ts_flex\n!pip install tsflex --no-index --find-links=file:///kaggle/input/tsflex/ts_flex ","metadata":{"execution":{"iopub.status.busy":"2023-05-31T15:50:07.131018Z","iopub.execute_input":"2023-05-31T15:50:07.132190Z","iopub.status.idle":"2023-05-31T15:50:23.192280Z","shell.execute_reply.started":"2023-05-31T15:50:07.132140Z","shell.execute_reply":"2023-05-31T15:50:23.190912Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"colorama-0.4.6-py2.py3-none-any.whl\ndill-0.3.6-py3-none-any.whl\nmultiprocess-0.70.14-py38-none-any.whl\nnumpy-1.24.3-cp38-cp38-win_amd64.whl\npandas-1.5.3-cp38-cp38-win_amd64.whl\npython_dateutil-2.8.2-py2.py3-none-any.whl\npytz-2023.3-py2.py3-none-any.whl\nsix-1.16.0-py2.py3-none-any.whl\ntqdm-4.65.0-py3-none-any.whl\ntsflex-0.3.0-py3-none-any.whl\nLooking in links: file:///kaggle/input/tsflex/ts_flex\nProcessing /kaggle/input/tsflex/ts_flex/tsflex-0.3.0-py3-none-any.whl\nRequirement already satisfied: numpy<2.0.0,>=1.21.5 in /opt/conda/lib/python3.10/site-packages (from tsflex) (1.23.5)\nRequirement already satisfied: multiprocess<0.71.0,>=0.70.12 in /opt/conda/lib/python3.10/site-packages (from tsflex) (0.70.14)\nRequirement already satisfied: dill<0.4.0,>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from tsflex) (0.3.6)\nRequirement already satisfied: tqdm<5.0.0,>=4.62.3 in /opt/conda/lib/python3.10/site-packages (from tsflex) (4.64.1)\nRequirement already satisfied: pandas<2.0.0,>=1.3.5 in /opt/conda/lib/python3.10/site-packages (from tsflex) (1.5.3)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.0.0,>=1.3.5->tsflex) (2023.3)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.0.0,>=1.3.5->tsflex) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas<2.0.0,>=1.3.5->tsflex) (1.16.0)\nInstalling collected packages: tsflex\nSuccessfully installed tsflex-0.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom imblearn.over_sampling import SMOTE\nimport scipy.stats as ss\nfrom tsflex.features import MultipleFeatureDescriptors, FeatureCollection, FeatureDescriptor\nfrom tsflex.features.utils import make_robust\nimport warnings\n","metadata":{"papermill":{"duration":1.340262,"end_time":"2023-05-14T06:28:36.731675","exception":false,"start_time":"2023-05-14T06:28:35.391413","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-31T15:50:50.822816Z","iopub.execute_input":"2023-05-31T15:50:50.823894Z","iopub.status.idle":"2023-05-31T15:50:52.470301Z","shell.execute_reply.started":"2023-05-31T15:50:50.823845Z","shell.execute_reply":"2023-05-31T15:50:52.469006Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In addition, **tdcsfog_metadata.csv identifies** each series in the tdcsfog dataset by **a unique Subject, Visit, Test, and Medication condition**.","metadata":{"papermill":{"duration":0.024886,"end_time":"2023-05-14T06:28:37.543932","exception":false,"start_time":"2023-05-14T06:28:37.519046","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# tdcsfog metadata file\ntdcsfog_metadata = pd.read_csv(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/defog_metadata.csv\")\ntdcsfog_metadata.head(5)","metadata":{"papermill":{"duration":0.050176,"end_time":"2023-05-14T06:28:37.619958","exception":false,"start_time":"2023-05-14T06:28:37.569782","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-31T15:51:07.977360Z","iopub.execute_input":"2023-05-31T15:51:07.978083Z","iopub.status.idle":"2023-05-31T15:51:08.025491Z","shell.execute_reply.started":"2023-05-31T15:51:07.978047Z","shell.execute_reply":"2023-05-31T15:51:08.024374Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"           Id Subject  Visit Medication\n0  02ab235146  e1f62e      2         on\n1  02ea782681  ae2d35      2         on\n2  06414383cf  8c1f5e      2        off\n3  092b4c1819  2874c5      1        off\n4  0a900ed8a2  0e3d49      2         on","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Subject</th>\n      <th>Visit</th>\n      <th>Medication</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>02ab235146</td>\n      <td>e1f62e</td>\n      <td>2</td>\n      <td>on</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>02ea782681</td>\n      <td>ae2d35</td>\n      <td>2</td>\n      <td>on</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>06414383cf</td>\n      <td>8c1f5e</td>\n      <td>2</td>\n      <td>off</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>092b4c1819</td>\n      <td>2874c5</td>\n      <td>1</td>\n      <td>off</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0a900ed8a2</td>\n      <td>0e3d49</td>\n      <td>2</td>\n      <td>on</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Initialize the extraction pipeline","metadata":{"papermill":{"duration":0.02634,"end_time":"2023-05-14T06:28:38.924186","exception":false,"start_time":"2023-05-14T06:28:38.897846","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**Pipeline hyperparameters**","metadata":{}},{"cell_type":"code","source":"# Method of feature extraction, either concat all data and run feature extraction on that = whole_dataset (assumes that all data is chronologically ordered to some extent)\n# or perform feature extraction file by file and collect the results = \"individual files\"\nmethod = \"whole_dataset\"\n\n# The window label decides to what timestamp the results for a window are tied. E.g. middle means that the features generated by a window will be added to the row\n# containing the timestap in the middle of the window\nwindow_label = \"middle\"\n\n# The windows array decides the size of the window that will slide over the data\nwindows = [120]\n\n# The strides array decides with what size steps the window is going to slide over the data\nstrides = [1] \n\n# The columns that the features will be extracted from\nseries_names = [\"AccV\", \"AccML\", \"AccAP\"]\n\nprint(\"\\nSettings~ \\nWindow_size: \" + str(windows[0]) +\"\\nWindow_label: \" + str(window_label) + \"\\nMethod: \" + str(method) + \"\\nSeries_names: \" + str(series_names))","metadata":{"execution":{"iopub.status.busy":"2023-05-31T15:51:09.838472Z","iopub.execute_input":"2023-05-31T15:51:09.838881Z","iopub.status.idle":"2023-05-31T15:51:09.848179Z","shell.execute_reply.started":"2023-05-31T15:51:09.838851Z","shell.execute_reply":"2023-05-31T15:51:09.846822Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\nSettings~ \nWindow_size: 120\nWindow_label: middle\nMethod: whole_dataset\nSeries_names: ['AccV', 'AccML', 'AccAP']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tsflex feature collection pipeline\n\n# The funcs for the function set used to extract the data features\ndef slope(x): return (x[-1] - x[0]) / x[0] if x[0] else 0\ndef abs_diff_mean(x): return np.mean(np.abs(x[1:] - x[:-1])) if len(x) > 1 else 0\ndef diff_std(x): return np.std(x[1:] - x[:-1]) if len(x) > 1 else 0\n\n\n\n# funcs = [make_robust(f) for f in [np.median, np.min,np.var, np.max, np.std, np.mean]]\nfuncs = [make_robust(f) for f in [np.min,np.var, np.max, np.std, np.mean, slope, ss.skew, ss.kurtosis, abs_diff_mean, diff_std, np.sum]]\n\nfc_train = FeatureCollection(\n    MultipleFeatureDescriptors(\n          functions=funcs,\n          series_names=series_names,\n          windows=windows,\n          strides=strides[0],\n    )\n)\n\n# # Specifically for the dependent variables\n# npmean = make_robust(np.mean)\n\n# fc_train.add(FeatureDescriptor(npmean, \"StartHesitation\", windows[0], strides[0]))\n# fc_train.add(FeatureDescriptor(npmean, \"Walking\", windows[0], strides[0]))\n# fc_train.add(FeatureDescriptor(npmean, \"Turn\", windows[0], strides[0]))","metadata":{"papermill":{"duration":18.008288,"end_time":"2023-05-14T06:28:56.959744","exception":false,"start_time":"2023-05-14T06:28:38.951456","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-31T16:00:58.326577Z","iopub.execute_input":"2023-05-31T16:00:58.326993Z","iopub.status.idle":"2023-05-31T16:00:58.339295Z","shell.execute_reply.started":"2023-05-31T16:00:58.326963Z","shell.execute_reply":"2023-05-31T16:00:58.337822Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def extract_features(method = \"whole_dataset\", df_list = None, window_label = \"middle\", fc = None, test = False):\n    \"\"\"\n    method: Method of feature extraction (perform on a file-per-file basis or on the entire set)\n    df_list: The lists that contain the tdcsfog and defog dataframes\n    window_label: The id that the extracted features are tied to for every window\n    fc: The feature extraction pipeline\n    test: Whether this is the test phase or not.\"\"\"\n    \n    defog_tot = pd.DataFrame()\n    tdcsfog_tot = pd.DataFrame()\n\n    if len(df_list) <= 1:\n        raise Exception(\"Failed to pass the entire dataset\")\n        \n    # First all the files will be concatenated and subsequently ts_flex wil perform feature extraction\n    if method == \"whole_dataset\":\n        \n        # Collect all files within the defog and tdcsfog folders and concatenate those\n        for defog in df_list[0]: \n            defog_tot = pd.concat([defog_tot,defog],ignore_index = True)\n        for tdcsfog in df_list[1]:\n            tdcsfog_tot = pd.concat([tdcsfog_tot,tdcsfog],ignore_index = True)\n\n        # Concatenate the total tdcsfog and defog sets and continue under the name defog_test\n        defog_tot = pd.concat([tdcsfog_tot, defog_tot],ignore_index = True)\n        \n        # Reset the index for the feature collection\n        defog_tot = defog_tot.reset_index(drop = True)\n        defog_tot[\"Time\"] = list(defog_tot.index.values)\n\n        df_feats = fc.calculate(data=[defog_tot], window_idx=window_label, approve_sparsity=True, return_df=True, show_progress = True)\n        index = np.ones(len(df_feats), dtype=int)\n\n        \n        if window_label == 'middle':\n                # Repeat the first and last row window/2 times to generate a set of the same size as the input\n                index[[0, -1]] = windows[0] / 2 + 1\n        if window_label == 'end':\n                # Repeat the first row window times to generate a set of the same size as the input\n                index[[0]] = windows[0] + 1\n        if window_label == 'begin':\n                # Repeat the last row window times to generate a set of the same size as the input\n                index[[-1]] = windows[0] + 1\n        \n        df_feats = df_feats.iloc[np.arange(len(df_feats)).repeat(index)].reset_index(drop = True)\n\n        # The input and output should have the same length and ID's\n        assert(len(defog_tot) == len(df_feats))\n        \n        if(test):\n            df_feats['Id'] = defog_tot['Id']\n            defog_tot = df_feats.merge(defog_tot.drop(columns = [\"Time\"]), on = \"Id\")\n        else: \n            defog_tot = df_feats.join(defog_tot.drop(columns = [\"Time\"]))\n        \n        return defog_tot\n\n    # Ts_flex performs feature extraction per file and concatenates outputs\n    if method == \"individual_files\":\n\n        for idx, defog in enumerate(df_list[0]): \n            \n            defog = defog.reset_index(drop = True)\n            \n            df_feats = fc.calculate(data=[defog], window_idx=window_label, approve_sparsity=True, return_df=True)\n            index = np.ones(len(df_feats), dtype=int)\n\n\n            if window_label == 'middle':\n                # Repeat the first and last row window/2 times to generate a set of the same size as the input\n                index[[0, -1]] = windows[0] / 2 + 1\n            if window_label == 'end':\n                # Repeat the first row window times to generate a set of the same size as the input\n                index[[0]] = windows[0] + 1\n            if window_label == 'begin':\n                # Repeat the last row window times to generate a set of the same size as the input\n                index[[-1]] = windows[0] + 1\n\n            df_feats = df_feats.iloc[np.arange(len(df_feats)).repeat(index)].reset_index(drop = True)\n            \n            assert(len(df_feats) == len(defog))\n            \n            if test:  \n                df_feats['Id'] = defog['Id']\n                df_feats = df_feats.merge(defog.drop(columns = [\"Time\"]), on = \"Id\")\n            else:\n                df_feats = df_feats.join(defog.drop(columns = [\"Time\"]))\n                \n            defog_tot = pd.concat([defog_tot,df_feats],ignore_index = True)\n\n        for idx, tdcsfog in enumerate(df_list[1]): \n            \n            tdcsfog = tdcsfog.reset_index(drop = True)\n                \n            df_feats = fc.calculate(data=[tdcsfog], window_idx=window_label, approve_sparsity=True, return_df=True)\n            index = np.ones(len(df_feats), dtype=int)\n\n            if window_label == 'middle':\n                # Repeat the first and last row window/2 times to generate a set of the same size as the input\n                index[[0, -1]] = windows[0] / 2 + 1\n            if window_label == 'end':\n                # Repeat the first row window times to generate a set of the same size as the input\n                index[[0]] = windows[0] + 1\n            if window_label == 'begin':\n                # Repeat the last row window times to generate a set of the same size as the input\n                index[[-1]] = windows[0] + 1\n\n            df_feats = df_feats.iloc[np.arange(len(df_feats)).repeat(index)].reset_index(drop = True)\n            \n            assert(len(df_feats) == len(tdcsfog))\n            \n            if test:\n                df_feats['Id'] = tdcsfog['Id']\n                df_feats = df_feats.merge(tdcsfog.drop(columns = [\"Time\"]), on = \"Id\")\n            else: \n                df_feats = df_feats.join(tdcsfog.drop(columns = [\"Time\"]))\n                \n            tdcsfog_tot = pd.concat([tdcsfog_tot,df_feats],ignore_index = True)\n            \n        defog_tot = pd.concat([tdcsfog_tot, defog_tot], ignore_index = True) \n               \n        return defog_tot\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T16:02:24.462055Z","iopub.execute_input":"2023-05-31T16:02:24.462513Z","iopub.status.idle":"2023-05-31T16:02:24.491986Z","shell.execute_reply.started":"2023-05-31T16:02:24.462478Z","shell.execute_reply":"2023-05-31T16:02:24.490676Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Collect the Defog and Tdcsfog train files","metadata":{}},{"cell_type":"code","source":"# Set the directory path to the folder containing the CSV files.\ntdcsfog_path = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog'\n\n# Initialize an empty list to store the dataframes.\ntdcsfog_list = []\n\n\n# Loop through each file in the directory and read it into a dataframe.\nfor file_name in os.listdir(tdcsfog_path):\n    if file_name.endswith('.csv'):\n        file_path = os.path.join(tdcsfog_path, file_name)\n        file = pd.read_csv(file_path)\n        file.Time = file.Time # / (len(file) - 1)\n        tdcsfog_list.append(file)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T16:01:02.405342Z","iopub.execute_input":"2023-05-31T16:01:02.406627Z","iopub.status.idle":"2023-05-31T16:01:12.976786Z","shell.execute_reply.started":"2023-05-31T16:01:02.406576Z","shell.execute_reply":"2023-05-31T16:01:12.975739Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Set the directory path to the folder containing the CSV files.\ndefog_path = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog'\n\n# Initialize an empty list to store the dataframes.\ndefog_list = []\n\n# Loop through each file in the directory and read it into a dataframe.\nfor file_name in os.listdir(defog_path):\n    if file_name.endswith('.csv'):\n        file_path = os.path.join(defog_path, file_name)\n        file = pd.read_csv(file_path)\n        file.Time = file.Time # / (len(file) - 1)\n        file = file[(file['Task'] == 1) & (file['Valid'] == 1)]\n        file = file.drop(columns = ['Task','Valid'])\n        defog_list.append(file)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T16:01:12.978540Z","iopub.execute_input":"2023-05-31T16:01:12.978902Z","iopub.status.idle":"2023-05-31T16:01:29.528672Z","shell.execute_reply.started":"2023-05-31T16:01:12.978873Z","shell.execute_reply":"2023-05-31T16:01:29.527296Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Set the training hyperparameters\n","metadata":{}},{"cell_type":"code","source":"# Input training data\ndf_list_train = [defog_list,tdcsfog_list]\n\n# Switch for train and test modes (mainly beacuse of ID's)\ntest = False","metadata":{"execution":{"iopub.status.busy":"2023-05-31T16:02:27.932732Z","iopub.execute_input":"2023-05-31T16:02:27.933510Z","iopub.status.idle":"2023-05-31T16:02:27.938694Z","shell.execute_reply.started":"2023-05-31T16:02:27.933444Z","shell.execute_reply":"2023-05-31T16:02:27.937769Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Extract features from the training data","metadata":{}},{"cell_type":"code","source":"train_features = extract_features(method = method, df_list = df_list_train, window_label = window_label, fc = fc_train, test = False)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T16:02:29.427488Z","iopub.execute_input":"2023-05-31T16:02:29.428290Z","iopub.status.idle":"2023-05-31T16:02:54.632721Z","shell.execute_reply.started":"2023-05-31T16:02:29.428247Z","shell.execute_reply":"2023-05-31T16:02:54.631482Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62d77a9faee644a4a844152c14a9d16a"}},"metadata":{}}]},{"cell_type":"markdown","source":"It is better to reduce the memory usage. Reference: [Reducing DataFrame memory size by ~65%](https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65)","metadata":{"papermill":{"duration":0.027012,"end_time":"2023-05-14T06:28:57.01431","exception":false,"start_time":"2023-05-14T06:28:56.987298","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def reduce_memory_usage(df):\n    \n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype.name\n        if ((col_type != 'datetime64[ns]') & (col_type != 'category')):\n            if (col_type != 'object'):\n                c_min = df[col].min()\n                c_max = df[col].max()\n\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)\n\n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        pass\n            else:\n                df[col] = df[col].astype('category')\n    mem_usg = df.memory_usage().sum() / 1024 ** 2 \n    print(\"Memory usage became: \",mem_usg,\" MB\")\n    \n    return df","metadata":{"papermill":{"duration":0.050491,"end_time":"2023-05-14T06:28:57.092322","exception":false,"start_time":"2023-05-14T06:28:57.041831","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-29T17:34:52.531933Z","iopub.execute_input":"2023-05-29T17:34:52.532352Z","iopub.status.idle":"2023-05-29T17:34:52.548591Z","shell.execute_reply.started":"2023-05-29T17:34:52.532302Z","shell.execute_reply":"2023-05-29T17:34:52.547363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_features = reduce_memory_usage(train_features)","metadata":{"papermill":{"duration":0.635449,"end_time":"2023-05-14T06:28:57.755209","exception":false,"start_time":"2023-05-14T06:28:57.11976","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-29T17:34:54.125908Z","iopub.execute_input":"2023-05-29T17:34:54.126539Z","iopub.status.idle":"2023-05-29T17:34:54.130730Z","shell.execute_reply.started":"2023-05-29T17:34:54.126507Z","shell.execute_reply":"2023-05-29T17:34:54.129615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features.describe()","metadata":{"papermill":{"duration":3.461726,"end_time":"2023-05-14T06:29:01.244355","exception":false,"start_time":"2023-05-14T06:28:57.782629","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-29T17:34:55.120301Z","iopub.execute_input":"2023-05-29T17:34:55.120737Z","iopub.status.idle":"2023-05-29T17:34:55.319279Z","shell.execute_reply.started":"2023-05-29T17:34:55.120705Z","shell.execute_reply":"2023-05-29T17:34:55.318449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Dataset\n\nFirst, we need to **split the data into input features (i.e. \"Time\", \"AccV\", \"AccML\", and \"AccAP\") and target variables (i.e. \"StartHesitation\", \"Turn\", and \"Walking\")**. We can do this using the .iloc method to select the appropriate columns.","metadata":{"papermill":{"duration":0.036051,"end_time":"2023-05-14T06:34:24.891554","exception":false,"start_time":"2023-05-14T06:34:24.855503","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Use smote to create synthetic data\n\n# smote = SMOTE(random_state = 4, k_neighbors=100)\n# X_syn, y_syn = smote.fit_resample(X_merged, merged['label'])\n\n\n# Create Synthetic dataset\n# syn = pd.concat([X_syn,y_syn.to_frame(name = \"label\")], axis=1)\n# syn[\"Turn\"], syn[\"Walking\"], syn[\"StartHesitation\"] = (syn[\"label\"] == 1).astype(int), (syn[\"label\"] == 2).astype(int), (syn[\"label\"] == 3).astype(int)\n\n# tot = pd.concat([merged,syn])\n# tot = tot.sort_values(\"Time\",ignore_index = True)\n\n# Normalize time\n# tot[\"Time\"] = tot[\"Time\"] / (len(tot) - 1)","metadata":{"papermill":{"duration":0.208778,"end_time":"2023-05-14T06:34:25.136244","exception":false,"start_time":"2023-05-14T06:34:24.927466","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-25T20:14:32.544673Z","iopub.execute_input":"2023-05-25T20:14:32.545685Z","iopub.status.idle":"2023-05-25T20:14:32.551376Z","shell.execute_reply.started":"2023-05-25T20:14:32.545643Z","shell.execute_reply":"2023-05-25T20:14:32.550340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data = np.array([train_features['Walking__mean__w='+ str(windows[0])], train_features['StartHesitation__mean__w=' + str(windows[0])], train_features['Turn__mean__w='+ str(windows[0])]])\n\n# labels = np.argmax(data, axis = 0)\n# sums = np.sum(data, axis = 0)\n# labels = np.where(sums == 0 , 5, labels)\n\ny1 = train_features['StartHesitation']  # target variable for StartHesitation\ny2 = train_features['Turn']  # target variable for Turn\ny3 = train_features['Walking']  # target variable for Walking\n\ntrain_features = train_features.drop(columns = [\"StartHesitation\",\"Turn\",\"Walking\"])\n\n# Change this by hand if you want to try more features\nX_tot = pd.concat([train_features.iloc[:, :(len(funcs) * len(series_names))],train_features.iloc[:, -len(series_names):]], axis = 1, ignore_index = False)\nprint(X_tot.columns)\n\n# train_features['Walking'] = np.where(labels == 0 , 1, 0)\n# train_features['StartHesitation'] = np.where(labels == 1 , 1, 0)\n# train_features['Turn'] = np.where(labels == 2 , 1, 0)\n\n\n# y1 = train_features['StartHesitation']  # target variable for StartHesitation\n# y2 = train_features['Turn']  # target variable for Turn\n# y3 = train_features['Walking']  # target variable for Walking\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T16:03:32.485447Z","iopub.execute_input":"2023-05-31T16:03:32.485924Z","iopub.status.idle":"2023-05-31T16:03:32.511340Z","shell.execute_reply.started":"2023-05-31T16:03:32.485889Z","shell.execute_reply":"2023-05-31T16:03:32.509569Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Index(['AccAP__abs_diff_mean__w=120', 'AccAP__amax__w=120',\n       'AccAP__amin__w=120', 'AccAP__diff_std__w=120',\n       'AccAP__kurtosis__w=120', 'AccAP__mean__w=120', 'AccAP__skew__w=120',\n       'AccAP__slope__w=120', 'AccAP__std__w=120', 'AccAP__sum__w=120',\n       'AccAP__var__w=120', 'AccML__abs_diff_mean__w=120',\n       'AccML__amax__w=120', 'AccML__amin__w=120', 'AccML__diff_std__w=120',\n       'AccML__kurtosis__w=120', 'AccML__mean__w=120', 'AccML__skew__w=120',\n       'AccML__slope__w=120', 'AccML__std__w=120', 'AccML__sum__w=120',\n       'AccML__var__w=120', 'AccV__abs_diff_mean__w=120', 'AccV__amax__w=120',\n       'AccV__amin__w=120', 'AccV__diff_std__w=120', 'AccV__kurtosis__w=120',\n       'AccV__mean__w=120', 'AccV__skew__w=120', 'AccV__slope__w=120',\n       'AccV__std__w=120', 'AccV__sum__w=120', 'AccV__var__w=120', 'AccV',\n       'AccML', 'AccAP'],\n      dtype='object')\nIndex(['AccAP__abs_diff_mean__w=120', 'AccAP__amax__w=120',\n       'AccAP__amin__w=120', 'AccAP__diff_std__w=120',\n       'AccAP__kurtosis__w=120', 'AccAP__mean__w=120', 'AccAP__skew__w=120',\n       'AccAP__slope__w=120', 'AccAP__std__w=120', 'AccAP__sum__w=120',\n       'AccAP__var__w=120', 'AccML__abs_diff_mean__w=120',\n       'AccML__amax__w=120', 'AccML__amin__w=120', 'AccML__diff_std__w=120',\n       'AccML__kurtosis__w=120', 'AccML__mean__w=120', 'AccML__skew__w=120',\n       'AccML__slope__w=120', 'AccML__std__w=120', 'AccML__sum__w=120',\n       'AccML__var__w=120', 'AccV__abs_diff_mean__w=120', 'AccV__amax__w=120',\n       'AccV__amin__w=120', 'AccV__diff_std__w=120', 'AccV__kurtosis__w=120',\n       'AccV__mean__w=120', 'AccV__skew__w=120', 'AccV__slope__w=120',\n       'AccV__std__w=120', 'AccV__sum__w=120', 'AccV__var__w=120', 'AccV',\n       'AccML', 'AccAP'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Most of the target variables are 0. We had better **create each balanced dataset with the target variables of 0 and 1 equally**.","metadata":{"papermill":{"duration":0.035961,"end_time":"2023-05-14T06:34:25.208958","exception":false,"start_time":"2023-05-14T06:34:25.172997","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Find the positions of y1 where it equals 0.\ny1_zeros = np.where(y1 == 0)[0]\ny1_ones = np.where(y1 == 1)[0]\n\n# Choose the same number of samples with y1 == 1 as there are with y1 == 0.\nnum1_ones = (y1 == 1).sum()\nnp.random.seed(42)\ny1_zeros = np.random.choice(np.where(y1 == 0)[0], size = num1_ones, replace = False)\n\n# Combine the positions of y1 == 0 and y1 == 1.\ny1_balanced_idxs = np.sort(np.concatenate([y1_zeros, y1_ones]))\n\n# Use the balanced indices to get the corresponding rows of X and y1.\nX1_balanced = X_tot.iloc[y1_balanced_idxs, :]\ny1_balanced = y1.iloc[y1_balanced_idxs]","metadata":{"papermill":{"duration":0.760961,"end_time":"2023-05-14T06:34:26.007999","exception":false,"start_time":"2023-05-14T06:34:25.247038","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-31T16:04:04.828796Z","iopub.execute_input":"2023-05-31T16:04:04.829244Z","iopub.status.idle":"2023-05-31T16:04:04.851941Z","shell.execute_reply.started":"2023-05-31T16:04:04.829213Z","shell.execute_reply":"2023-05-31T16:04:04.850704Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Find the positions of y2 where it equals 0.\ny2_zeros = np.where(y2 == 0)[0]\ny2_ones = np.where(y2 == 1)[0]\n\n# Choose the same number of samples with y2 == 1 as there are with y2 == 0.\nnum2_ones = (y2 == 1).sum()\nnp.random.seed(42)\ny2_zeros = np.random.choice(np.where(y2 == 0)[0], size = num2_ones, replace = False)\n\n# Combine the positions of y2 == 0 and y2 == 1.\ny2_balanced_idxs = np.sort(np.concatenate([y2_zeros, y2_ones]))\n\n# Use the balanced indices to get the corresponding rows of X and y1.\nX2_balanced = X_tot.iloc[y2_balanced_idxs, :]\ny2_balanced = y2.iloc[y2_balanced_idxs]","metadata":{"papermill":{"duration":1.218739,"end_time":"2023-05-14T06:34:27.263905","exception":false,"start_time":"2023-05-14T06:34:26.045166","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-31T16:04:06.077899Z","iopub.execute_input":"2023-05-31T16:04:06.078337Z","iopub.status.idle":"2023-05-31T16:04:06.094927Z","shell.execute_reply.started":"2023-05-31T16:04:06.078285Z","shell.execute_reply":"2023-05-31T16:04:06.093705Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Find the positions of y3 where it equals 0.\ny3_zeros = np.where(y3 == 0)[0]\ny3_ones = np.where(y3 == 1)[0]\n\n# Choose the same number of samples with y3 == 1 as there are with y3 == 0.\nnum3_ones = (y3 == 1).sum()\nnp.random.seed(42)\ny3_zeros = np.random.choice(np.where(y3 == 0)[0], size = num3_ones, replace = False)\n\n# Combine the positions of y3 == 0 and y3 == 1.\ny3_balanced_idxs = np.sort(np.concatenate([y3_zeros, y3_ones]))\n\n# Use the balanced indices to get the corresponding rows of X and y3.\nX3_balanced = X_tot.iloc[y3_balanced_idxs, :]\ny3_balanced = y3.iloc[y3_balanced_idxs]","metadata":{"papermill":{"duration":0.801883,"end_time":"2023-05-14T06:34:28.102399","exception":false,"start_time":"2023-05-14T06:34:27.300516","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-31T16:04:07.622707Z","iopub.execute_input":"2023-05-31T16:04:07.623142Z","iopub.status.idle":"2023-05-31T16:04:07.636515Z","shell.execute_reply.started":"2023-05-31T16:04:07.623111Z","shell.execute_reply":"2023-05-31T16:04:07.634661Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Next, we can **split the data into training and testing sets using the train_test_split function from scikit-learn**.","metadata":{"papermill":{"duration":0.036394,"end_time":"2023-05-14T06:34:28.17924","exception":false,"start_time":"2023-05-14T06:34:28.142846","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1_balanced, y1_balanced, test_size = 0.2, random_state = 42)\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2_balanced, y2_balanced, test_size = 0.2, random_state = 42)\nX3_train, X3_test, y3_train, y3_test = train_test_split(X3_balanced, y3_balanced, test_size = 0.2, random_state = 42)","metadata":{"papermill":{"duration":1.062844,"end_time":"2023-05-14T06:34:29.279157","exception":false,"start_time":"2023-05-14T06:34:28.216313","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-28T18:59:04.938459Z","iopub.execute_input":"2023-05-28T18:59:04.938952Z","iopub.status.idle":"2023-05-28T18:59:05.133702Z","shell.execute_reply.started":"2023-05-28T18:59:04.938908Z","shell.execute_reply":"2023-05-28T18:59:05.132442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we **standardize the independent variables**.","metadata":{"papermill":{"duration":0.035687,"end_time":"2023-05-14T06:34:29.350635","exception":false,"start_time":"2023-05-14T06:34:29.314948","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Standardize the independent variables.\nscaler1 = StandardScaler()\nX1_train = scaler1.fit_transform(X1_train)\nX1_test = scaler1.transform(X1_test)\n\nscaler2 = StandardScaler()\nX2_train = scaler2.fit_transform(X2_train)\nX2_test = scaler2.transform(X2_test)\n\nscaler3 = StandardScaler()\nX3_train = scaler3.fit_transform(X3_train)\nX3_test = scaler3.transform(X3_test)","metadata":{"papermill":{"duration":0.910372,"end_time":"2023-05-14T06:34:30.297294","exception":false,"start_time":"2023-05-14T06:34:29.386922","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-28T18:59:07.423118Z","iopub.execute_input":"2023-05-28T18:59:07.423557Z","iopub.status.idle":"2023-05-28T18:59:07.644014Z","shell.execute_reply.started":"2023-05-28T18:59:07.423521Z","shell.execute_reply":"2023-05-28T18:59:07.642979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create, Train, and Evaluate Model\n\nFinally, we can **create and train three separate models**, one for each target variable, using a suitable algorithm.\n       \n### This time we use **Random Forest Regressor instead of the Logistic Regression model**.\n\n**For a Logistic Regression model, please see [PD FOG Prediction Baseline by Logistic Regression](https://www.kaggle.com/code/gokifujiya/pd-fog-prediction-baseline-by-logistic-regression).**","metadata":{"papermill":{"duration":0.035684,"end_time":"2023-05-14T06:34:30.369288","exception":false,"start_time":"2023-05-14T06:34:30.333604","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#from sklearn.linear_model import LogisticRegression\nfrom sklearn import ensemble\n\n# Create three separate logistic regression models.\n#model1 = LogisticRegression()\n#model2 = LogisticRegression()\n#model3 = LogisticRegression()\n\n# Create three separate Random Forest Regressor models.\nmodel1 = ensemble.RandomForestRegressor(n_estimators = 100, max_depth = 7, n_jobs = -1, random_state = 42)\nmodel2 = ensemble.RandomForestRegressor(n_estimators = 100, max_depth = 7, n_jobs = -1, random_state = 42)\nmodel3 = ensemble.RandomForestRegressor(n_estimators = 100, max_depth = 7, n_jobs = -1, random_state = 42)\n\n# Train the models on the training data.\nmodel1.fit(X1_train, y1_train)\nmodel2.fit(X2_train, y2_train)\nmodel3.fit(X3_train, y3_train)\n\n# Evaluate the models on the test data.\nprint('R2 for StartHesitation:', model1.score(X1_test, y1_test))\nprint('R2 for Turn:', model2.score(X2_test, y2_test))\nprint('R2 for Walking:', model3.score(X3_test, y3_test))","metadata":{"papermill":{"duration":390.611171,"end_time":"2023-05-14T06:41:01.016047","exception":false,"start_time":"2023-05-14T06:34:30.404876","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-28T18:59:09.886530Z","iopub.execute_input":"2023-05-28T18:59:09.887025Z","iopub.status.idle":"2023-05-28T19:04:41.834408Z","shell.execute_reply.started":"2023-05-28T18:59:09.886985Z","shell.execute_reply":"2023-05-28T19:04:41.833108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Recreate Dataset and Training\n\n**For submission we should not split the datasets to keep the amount of data and to get a higher score.**","metadata":{"papermill":{"duration":0.035718,"end_time":"2023-05-14T06:41:01.088534","exception":false,"start_time":"2023-05-14T06:41:01.052816","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Standardize the independent variables.\nscaler1 = StandardScaler()\nX1_balanced = scaler1.fit_transform(X1_balanced)\n\nscaler2 = StandardScaler()\nX2_balanced = scaler2.fit_transform(X2_balanced)\n\nscaler3 = StandardScaler()\nX3_balanced = scaler3.fit_transform(X3_balanced)","metadata":{"papermill":{"duration":0.995912,"end_time":"2023-05-14T06:41:02.120305","exception":false,"start_time":"2023-05-14T06:41:01.124393","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-28T19:05:06.344300Z","iopub.execute_input":"2023-05-28T19:05:06.344781Z","iopub.status.idle":"2023-05-28T19:05:06.570187Z","shell.execute_reply.started":"2023-05-28T19:05:06.344722Z","shell.execute_reply":"2023-05-28T19:05:06.568873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.linear_model import LogisticRegression\nfrom sklearn import ensemble\n\n# Create three separate logistic regression models.\n#model1 = LogisticRegression()\n#model2 = LogisticRegression()\n#model3 = LogisticRegression()\n\n# Create three separate Random Forest Regressor models.\nmodel1 = ensemble.RandomForestRegressor(n_estimators = 100, max_depth = 7, n_jobs = -1, random_state = 42)\nmodel2 = ensemble.RandomForestRegressor(n_estimators = 100, max_depth = 7, n_jobs = -1, random_state = 42)\nmodel3 = ensemble.RandomForestRegressor(n_estimators = 100, max_depth = 7, n_jobs = -1, random_state = 42)\n\n# Train the models on the training data.\nmodel1.fit(X1_balanced, y1_balanced)\nmodel2.fit(X2_balanced, y2_balanced)\nmodel3.fit(X3_balanced, y3_balanced)","metadata":{"papermill":{"duration":409.134256,"end_time":"2023-05-14T06:47:51.291301","exception":false,"start_time":"2023-05-14T06:41:02.157045","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-28T19:05:22.713867Z","iopub.execute_input":"2023-05-28T19:05:22.714333Z","iopub.status.idle":"2023-05-28T19:11:17.368245Z","shell.execute_reply.started":"2023-05-28T19:05:22.714297Z","shell.execute_reply":"2023-05-28T19:11:17.366809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Test Dataset","metadata":{"papermill":{"duration":0.036298,"end_time":"2023-05-14T06:47:51.363758","exception":false,"start_time":"2023-05-14T06:47:51.32746","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Set the directory path to the folder containing the CSV files.\ntdcsfog_test_path = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/tdcsfog'\n\n# Initialize an empty list to store the dataframes.\ntdcsfog_test_list = []\n\n# Loop through each file in the directory and read it into a dataframe.\nfor file_name in os.listdir(tdcsfog_test_path):\n    if file_name.endswith('.csv'):\n        file_path = os.path.join(tdcsfog_test_path, file_name)\n        file = pd.read_csv(file_path)\n        file['Id'] = file_name[:-4] + '_' + file['Time'].apply(str)\n        file.Time = file.Time \n        tdcsfog_test_list.append(file)","metadata":{"papermill":{"duration":0.1014,"end_time":"2023-05-14T06:47:51.501918","exception":false,"start_time":"2023-05-14T06:47:51.400518","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-28T19:11:28.761052Z","iopub.execute_input":"2023-05-28T19:11:28.761521Z","iopub.status.idle":"2023-05-28T19:11:28.794413Z","shell.execute_reply.started":"2023-05-28T19:11:28.761484Z","shell.execute_reply":"2023-05-28T19:11:28.792955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the directory path to the folder containing the CSV files.\ndefog_test_path = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/defog'\n\n# Initialize an empty list to store the dataframes.\ndefog_test_list = []\n\n# Loop through each file in the directory and read it into a dataframe.\nfor file_name in os.listdir(defog_test_path):\n    if file_name.endswith('.csv'):\n        file_path = os.path.join(defog_test_path, file_name)\n        file = pd.read_csv(file_path)\n        file['Id'] = file_name[:-4] + '_' + file['Time'].apply(str)\n        file.Time = file.Time\n        defog_test_list.append(file)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T19:11:31.390151Z","iopub.execute_input":"2023-05-28T19:11:31.390644Z","iopub.status.idle":"2023-05-28T19:11:31.838480Z","shell.execute_reply.started":"2023-05-28T19:11:31.390599Z","shell.execute_reply":"2023-05-28T19:11:31.837168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def slope(x): return (x[-1] - x[0]) / x[0] if x[0] else 0\ndef abs_diff_mean(x): return np.mean(np.abs(x[1:] - x[:-1])) if len(x) > 1 else 0\ndef diff_std(x): return np.std(x[1:] - x[:-1]) if len(x) > 1 else 0\n\n\n\nfuncs = [make_robust(f) for f in [np.min,np.var, np.max, np.std, np.mean, slope, ss.skew, ss.kurtosis, abs_diff_mean, diff_std, np.sum,]]\n\nfc_test = FeatureCollection(\n    MultipleFeatureDescriptors(\n          functions=funcs,\n          series_names=[\"AccV\", \"AccML\", \"AccAP\"],\n          windows=windows,\n          strides=strides[0],\n    )\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-28T19:11:35.916667Z","iopub.execute_input":"2023-05-28T19:11:35.917199Z","iopub.status.idle":"2023-05-28T19:11:35.930441Z","shell.execute_reply.started":"2023-05-28T19:11:35.917157Z","shell.execute_reply":"2023-05-28T19:11:35.929289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set the testing hyperparameters","metadata":{}},{"cell_type":"code","source":"# Input testing data\ndf_list_test = [defog_test_list,tdcsfog_test_list]\n\n# Switch for train and test modes (mainly beacuse of ID's)\ntest = True","metadata":{"execution":{"iopub.status.busy":"2023-05-28T19:12:14.620048Z","iopub.execute_input":"2023-05-28T19:12:14.620564Z","iopub.status.idle":"2023-05-28T19:12:14.629875Z","shell.execute_reply.started":"2023-05-28T19:12:14.620524Z","shell.execute_reply":"2023-05-28T19:12:14.628205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_features = extract_features(method = method, df_list = df_list_test, window_label = window_label, fc = fc_test, test = test)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T19:12:31.525839Z","iopub.execute_input":"2023-05-28T19:12:31.526342Z","iopub.status.idle":"2023-05-28T19:15:39.576444Z","shell.execute_reply.started":"2023-05-28T19:12:31.526307Z","shell.execute_reply":"2023-05-28T19:15:39.574727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_features = reduce_memory_usage(test_features)","metadata":{"papermill":{"duration":0.686019,"end_time":"2023-05-14T06:47:53.049474","exception":false,"start_time":"2023-05-14T06:47:52.363455","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-28T19:15:56.259219Z","iopub.execute_input":"2023-05-28T19:15:56.259930Z","iopub.status.idle":"2023-05-28T19:15:56.269276Z","shell.execute_reply.started":"2023-05-28T19:15:56.259860Z","shell.execute_reply":"2023-05-28T19:15:56.267937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_features","metadata":{"papermill":{"duration":0.234153,"end_time":"2023-05-14T06:47:53.320407","exception":false,"start_time":"2023-05-14T06:47:53.086254","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-28T19:15:59.029121Z","iopub.execute_input":"2023-05-28T19:15:59.029961Z","iopub.status.idle":"2023-05-28T19:15:59.194872Z","shell.execute_reply.started":"2023-05-28T19:15:59.029914Z","shell.execute_reply":"2023-05-28T19:15:59.193443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{"papermill":{"duration":0.037342,"end_time":"2023-05-14T06:47:53.395359","exception":false,"start_time":"2023-05-14T06:47:53.358017","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Separate the dataset for the independent variables.\n# Change by hand\ntest_X = pd.concat([test_features.iloc[:, :(len(funcs) * len(series_names))],test_features.iloc[:, -len(series_names):]], axis = 1, ignore_index = False)\n\nprint(test_X)\n# Standardize the independent variables by a new scaler.\nscaler = StandardScaler()\ntest_X = scaler.fit_transform(test_X)\n\n# Get the predictions for the three models on the test data.\npred_y1 = model1.predict(test_X)\npred_y2 = model2.predict(test_X)\npred_y3 = model3.predict(test_X)\n\ntest_features['StartHesitation'] = pred_y1 # target variable for StartHesitation\ntest_features['Turn'] = pred_y2 # target variable for Turn\ntest_features['Walking'] = pred_y3 # target variable for Walking\n\nprint(test_features)","metadata":{"papermill":{"duration":1.047587,"end_time":"2023-05-14T06:47:54.480581","exception":false,"start_time":"2023-05-14T06:47:53.432994","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-28T19:23:24.081028Z","iopub.execute_input":"2023-05-28T19:23:24.081481Z","iopub.status.idle":"2023-05-28T19:23:25.724818Z","shell.execute_reply.started":"2023-05-28T19:23:24.081446Z","shell.execute_reply":"2023-05-28T19:23:25.723836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{"papermill":{"duration":0.037147,"end_time":"2023-05-14T06:47:54.556874","exception":false,"start_time":"2023-05-14T06:47:54.519727","status":"completed"},"tags":[]}},{"cell_type":"code","source":"submission = test_features.loc[:,['Id','StartHesitation','Turn','Walking']].fillna(0.0)\nsubmission","metadata":{"papermill":{"duration":0.220266,"end_time":"2023-05-14T06:47:54.817263","exception":false,"start_time":"2023-05-14T06:47:54.596997","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-28T19:23:42.910224Z","iopub.execute_input":"2023-05-28T19:23:42.910713Z","iopub.status.idle":"2023-05-28T19:23:43.043125Z","shell.execute_reply.started":"2023-05-28T19:23:42.910672Z","shell.execute_reply":"2023-05-28T19:23:43.041831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","metadata":{"papermill":{"duration":2.256699,"end_time":"2023-05-14T06:47:57.112488","exception":false,"start_time":"2023-05-14T06:47:54.855789","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-14T07:40:39.791585Z","iopub.execute_input":"2023-05-14T07:40:39.7924Z","iopub.status.idle":"2023-05-14T07:40:42.286661Z","shell.execute_reply.started":"2023-05-14T07:40:39.792362Z","shell.execute_reply":"2023-05-14T07:40:42.285704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save, Load, and Use Model\n\nTo save the trained Logistic Regression model, you can use the joblib library from the sklearn.externals module. This will save the model to a file in the current working directory. **To load the saved model later**, we can use the joblib.load() function.","metadata":{"papermill":{"duration":0.038824,"end_time":"2023-05-14T06:47:57.191566","exception":false,"start_time":"2023-05-14T06:47:57.152742","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import joblib\n\n# Save the model to disk.\njoblib.dump(model1, 'model1.joblib')\njoblib.dump(model2, 'model2.joblib')\njoblib.dump(model3, 'model3.joblib')\n\n# Load the saved models from disk.\nmodel1_loaded = joblib.load('model1.joblib')\nmodel2_loaded = joblib.load('model2.joblib')\nmodel3_loaded = joblib.load('model3.joblib')\n\n# Use the loaded models to make predictions on test data.\ny1_pred_loaded = model1_loaded.predict(test_X)\ny2_pred_loaded = model2_loaded.predict(test_X)\ny3_pred_loaded = model3_loaded.predict(test_X)","metadata":{"papermill":{"duration":1.274561,"end_time":"2023-05-14T06:47:58.505545","exception":false,"start_time":"2023-05-14T06:47:57.230984","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-14T07:40:42.289511Z","iopub.execute_input":"2023-05-14T07:40:42.290001Z","iopub.status.idle":"2023-05-14T07:40:43.569222Z","shell.execute_reply.started":"2023-05-14T07:40:42.289961Z","shell.execute_reply":"2023-05-14T07:40:43.567876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nIt is possible that **more features or more advanced machine learning algorithms** could improve the accuracy of the models. Additionally, it may be useful to **investigate other factors** that contribute to the occurrence of freezing of gait events, such as cognitive or environmental factors.","metadata":{"papermill":{"duration":0.037949,"end_time":"2023-05-14T06:47:58.581718","exception":false,"start_time":"2023-05-14T06:47:58.543769","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"I am a medical doctor working on **artificial intelligence (AI) for medicine**. At present AI is also widely used in the medical field. Particularly, AI performs in the healthcare sector following tasks: **image classification, object detection, semantic segmentation, GANs, text classification, etc**. **If you are interested in AI for medicine, please see my other notebooks.**","metadata":{"papermill":{"duration":0.037761,"end_time":"2023-05-14T06:47:58.657688","exception":false,"start_time":"2023-05-14T06:47:58.619927","status":"completed"},"tags":[]}}]}