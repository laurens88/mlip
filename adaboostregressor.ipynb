{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThe goal of this competition is to **detect freezing of gait (FOG)**, a debilitating symptom that afflicts many people **with Parkinson’s disease**. It is requred to **develop a machine learning model trained on data collected from a wearable 3D lower back sensor** to better understand **when and why FOG episodes occur**.","metadata":{"papermill":{"duration":0.026599,"end_time":"2023-05-14T06:28:35.319714","exception":false,"start_time":"2023-05-14T06:28:35.293115","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{"papermill":{"duration":0.024773,"end_time":"2023-05-14T06:28:35.368114","exception":false,"start_time":"2023-05-14T06:28:35.343341","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!ls ../input/tsflex/ts_flex\n!pip install tsflex --no-index --find-links=file:///kaggle/input/tsflex/ts_flex ","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:41:10.059675Z","iopub.execute_input":"2023-06-05T14:41:10.060039Z","iopub.status.idle":"2023-06-05T14:41:21.597681Z","shell.execute_reply.started":"2023-06-05T14:41:10.060016Z","shell.execute_reply":"2023-06-05T14:41:21.596022Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"colorama-0.4.6-py2.py3-none-any.whl\ndill-0.3.6-py3-none-any.whl\nmultiprocess-0.70.14-py38-none-any.whl\nnumpy-1.24.3-cp38-cp38-win_amd64.whl\npandas-1.5.3-cp38-cp38-win_amd64.whl\npython_dateutil-2.8.2-py2.py3-none-any.whl\npytz-2023.3-py2.py3-none-any.whl\nsix-1.16.0-py2.py3-none-any.whl\ntqdm-4.65.0-py3-none-any.whl\ntsflex-0.3.0-py3-none-any.whl\nLooking in links: file:///kaggle/input/tsflex/ts_flex\nProcessing /kaggle/input/tsflex/ts_flex/tsflex-0.3.0-py3-none-any.whl\nRequirement already satisfied: dill<0.4.0,>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from tsflex) (0.3.6)\nRequirement already satisfied: multiprocess<0.71.0,>=0.70.12 in /opt/conda/lib/python3.10/site-packages (from tsflex) (0.70.14)\nRequirement already satisfied: numpy<2.0.0,>=1.21.5 in /opt/conda/lib/python3.10/site-packages (from tsflex) (1.23.5)\nRequirement already satisfied: pandas<2.0.0,>=1.3.5 in /opt/conda/lib/python3.10/site-packages (from tsflex) (1.5.3)\nRequirement already satisfied: tqdm<5.0.0,>=4.62.3 in /opt/conda/lib/python3.10/site-packages (from tsflex) (4.64.1)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.0.0,>=1.3.5->tsflex) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.0.0,>=1.3.5->tsflex) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas<2.0.0,>=1.3.5->tsflex) (1.16.0)\nInstalling collected packages: tsflex\nSuccessfully installed tsflex-0.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom imblearn.over_sampling import SMOTE\nimport scipy.stats as ss\nfrom tsflex.features import MultipleFeatureDescriptors, FeatureCollection, FeatureDescriptor\nfrom tsflex.features.utils import make_robust\nimport warnings\n","metadata":{"papermill":{"duration":1.340262,"end_time":"2023-05-14T06:28:36.731675","exception":false,"start_time":"2023-05-14T06:28:35.391413","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T14:41:21.600294Z","iopub.execute_input":"2023-06-05T14:41:21.600671Z","iopub.status.idle":"2023-06-05T14:41:23.237458Z","shell.execute_reply.started":"2023-06-05T14:41:21.600637Z","shell.execute_reply":"2023-06-05T14:41:23.236001Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"In addition, **tdcsfog_metadata.csv identifies** each series in the tdcsfog dataset by **a unique Subject, Visit, Test, and Medication condition**.","metadata":{"papermill":{"duration":0.024886,"end_time":"2023-05-14T06:28:37.543932","exception":false,"start_time":"2023-05-14T06:28:37.519046","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# tdcsfog metadata file\ntdcsfog_metadata = pd.read_csv(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/defog_metadata.csv\")\ntdcsfog_metadata.head(5)","metadata":{"papermill":{"duration":0.050176,"end_time":"2023-05-14T06:28:37.619958","exception":false,"start_time":"2023-05-14T06:28:37.569782","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T14:41:23.239196Z","iopub.execute_input":"2023-06-05T14:41:23.240027Z","iopub.status.idle":"2023-06-05T14:41:23.291150Z","shell.execute_reply.started":"2023-06-05T14:41:23.239994Z","shell.execute_reply":"2023-06-05T14:41:23.290275Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"           Id Subject  Visit Medication\n0  02ab235146  e1f62e      2         on\n1  02ea782681  ae2d35      2         on\n2  06414383cf  8c1f5e      2        off\n3  092b4c1819  2874c5      1        off\n4  0a900ed8a2  0e3d49      2         on","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Subject</th>\n      <th>Visit</th>\n      <th>Medication</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>02ab235146</td>\n      <td>e1f62e</td>\n      <td>2</td>\n      <td>on</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>02ea782681</td>\n      <td>ae2d35</td>\n      <td>2</td>\n      <td>on</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>06414383cf</td>\n      <td>8c1f5e</td>\n      <td>2</td>\n      <td>off</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>092b4c1819</td>\n      <td>2874c5</td>\n      <td>1</td>\n      <td>off</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0a900ed8a2</td>\n      <td>0e3d49</td>\n      <td>2</td>\n      <td>on</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Initialize the extraction pipeline","metadata":{"papermill":{"duration":0.02634,"end_time":"2023-05-14T06:28:38.924186","exception":false,"start_time":"2023-05-14T06:28:38.897846","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**Pipeline hyperparameters**","metadata":{}},{"cell_type":"code","source":"# Method of feature extraction, either concat all data and run feature extraction on that = whole_dataset (assumes that all data is chronologically ordered to some extent)\n# or perform feature extraction file by file and collect the results = \"individual files\"\nmethod = \"whole_dataset\"\n\n# The window label decides to what timestamp the results for a window are tied. E.g. middle means that the features generated by a window will be added to the row\n# containing the timestap in the middle of the window\nwindow_label = \"middle\"\n\n# The windows array decides the size of the window that will slide over the data\nwindows = [220]\n\n# The strides array decides with what size steps the window is going to slide over the data\nstrides = [1] \n\n# The columns that the features will be extracted from\nseries_names = [\"AccV\", \"AccML\", \"AccAP\"]\n\nprint(\"\\nSettings~ \\nWindow_size: \" + str(windows[0]) +\"\\nWindow_label: \" + str(window_label) + \"\\nMethod: \" + str(method) + \"\\nSeries_names: \" + str(series_names))","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:41:23.292249Z","iopub.execute_input":"2023-06-05T14:41:23.293356Z","iopub.status.idle":"2023-06-05T14:41:23.301141Z","shell.execute_reply.started":"2023-06-05T14:41:23.293323Z","shell.execute_reply":"2023-06-05T14:41:23.299328Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\nSettings~ \nWindow_size: 220\nWindow_label: middle\nMethod: whole_dataset\nSeries_names: ['AccV', 'AccML', 'AccAP']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tsflex feature collection pipeline\n\n# The funcs for the function set used to extract the data features\ndef slope(x): return (x[-1] - x[0]) / x[0] if x[0] else 0\ndef abs_diff_mean(x): return np.mean(np.abs(x[1:] - x[:-1])) if len(x) > 1 else 0\ndef diff_std(x): return np.std(x[1:] - x[:-1]) if len(x) > 1 else 0\n\n\n\n# funcs = [make_robust(f) for f in [np.median, np.min,np.var, np.max, np.std, np.mean]]\nfuncs = [make_robust(f) for f in [np.min,np.var, np.max, np.std, np.mean, slope, ss.skew, ss.kurtosis, abs_diff_mean, diff_std, np.sum]]\n\nfc_train = FeatureCollection(\n    MultipleFeatureDescriptors(\n          functions=funcs,\n          series_names=series_names,\n          windows=windows,\n          strides=strides[0],\n    )\n)\n\n# # Specifically for the dependent variables\n# npmean = make_robust(np.mean)\n\n# fc_train.add(FeatureDescriptor(npmean, \"StartHesitation\", windows[0], strides[0]))\n# fc_train.add(FeatureDescriptor(npmean, \"Walking\", windows[0], strides[0]))\n# fc_train.add(FeatureDescriptor(npmean, \"Turn\", windows[0], strides[0]))","metadata":{"papermill":{"duration":18.008288,"end_time":"2023-05-14T06:28:56.959744","exception":false,"start_time":"2023-05-14T06:28:38.951456","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T14:41:23.305593Z","iopub.execute_input":"2023-06-05T14:41:23.306053Z","iopub.status.idle":"2023-06-05T14:41:23.318472Z","shell.execute_reply.started":"2023-06-05T14:41:23.306021Z","shell.execute_reply":"2023-06-05T14:41:23.316575Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def extract_features(method = \"whole_dataset\", df_list = None, window_label = \"middle\", fc = None, test = False):\n    \"\"\"\n    method: Method of feature extraction (perform on a file-per-file basis or on the entire set)\n    df_list: The lists that contain the tdcsfog and defog dataframes\n    window_label: The id that the extracted features are tied to for every window\n    fc: The feature extraction pipeline\n    test: Whether this is the test phase or not.\"\"\"\n    \n    defog_tot = pd.DataFrame()\n    tdcsfog_tot = pd.DataFrame()\n\n    if len(df_list) <= 1:\n        raise Exception(\"Failed to pass the entire dataset\")\n        \n    # First all the files will be concatenated and subsequently ts_flex wil perform feature extraction\n    if method == \"whole_dataset\":\n        \n        # Collect all files within the defog and tdcsfog folders and concatenate those\n        for defog in df_list[0]: \n            defog_tot = pd.concat([defog_tot,defog],ignore_index = True)\n        for tdcsfog in df_list[1]:\n            tdcsfog_tot = pd.concat([tdcsfog_tot,tdcsfog],ignore_index = True)\n\n        # Concatenate the total tdcsfog and defog sets and continue under the name defog_test\n        defog_tot = pd.concat([tdcsfog_tot, defog_tot],ignore_index = True)\n        \n        # Reset the index for the feature collection\n        defog_tot = defog_tot.reset_index(drop = True)\n        defog_tot[\"Time\"] = list(defog_tot.index.values)\n\n        df_feats = fc.calculate(data=[defog_tot], window_idx=window_label, approve_sparsity=True, return_df=True, show_progress = True)\n        index = np.ones(len(df_feats), dtype=int)\n\n        \n        if window_label == 'middle':\n                # Repeat the first and last row window/2 times to generate a set of the same size as the input\n                index[[0, -1]] = windows[0] / 2 + 1\n        if window_label == 'end':\n                # Repeat the first row window times to generate a set of the same size as the input\n                index[[0]] = windows[0] + 1\n        if window_label == 'begin':\n                # Repeat the last row window times to generate a set of the same size as the input\n                index[[-1]] = windows[0] + 1\n        \n        df_feats = df_feats.iloc[np.arange(len(df_feats)).repeat(index)].reset_index(drop = True)\n\n        # The input and output should have the same length and ID's\n        assert(len(defog_tot) == len(df_feats))\n        \n        if(test):\n            df_feats['Id'] = defog_tot['Id']\n            defog_tot = df_feats.merge(defog_tot.drop(columns = [\"Time\"]), on = \"Id\")\n        else: \n            defog_tot = df_feats.join(defog_tot.drop(columns = [\"Time\"]))\n        \n        return defog_tot\n\n    # Ts_flex performs feature extraction per file and concatenates outputs\n    if method == \"individual_files\":\n\n        for idx, defog in enumerate(df_list[0]): \n            \n            defog = defog.reset_index(drop = True)\n            \n            df_feats = fc.calculate(data=[defog], window_idx=window_label, approve_sparsity=True, return_df=True)\n            index = np.ones(len(df_feats), dtype=int)\n\n\n            if window_label == 'middle':\n                # Repeat the first and last row window/2 times to generate a set of the same size as the input\n                index[[0, -1]] = windows[0] / 2 + 1\n            if window_label == 'end':\n                # Repeat the first row window times to generate a set of the same size as the input\n                index[[0]] = windows[0] + 1\n            if window_label == 'begin':\n                # Repeat the last row window times to generate a set of the same size as the input\n                index[[-1]] = windows[0] + 1\n\n            df_feats = df_feats.iloc[np.arange(len(df_feats)).repeat(index)].reset_index(drop = True)\n            \n            assert(len(df_feats) == len(defog))\n            \n            if test:  \n                df_feats['Id'] = defog['Id']\n                df_feats = df_feats.merge(defog.drop(columns = [\"Time\"]), on = \"Id\")\n            else:\n                df_feats = df_feats.join(defog.drop(columns = [\"Time\"]))\n                \n            defog_tot = pd.concat([defog_tot,df_feats],ignore_index = True)\n\n        for idx, tdcsfog in enumerate(df_list[1]): \n            \n            tdcsfog = tdcsfog.reset_index(drop = True)\n                \n            df_feats = fc.calculate(data=[tdcsfog], window_idx=window_label, approve_sparsity=True, return_df=True)\n            index = np.ones(len(df_feats), dtype=int)\n\n            if window_label == 'middle':\n                # Repeat the first and last row window/2 times to generate a set of the same size as the input\n                index[[0, -1]] = windows[0] / 2 + 1\n            if window_label == 'end':\n                # Repeat the first row window times to generate a set of the same size as the input\n                index[[0]] = windows[0] + 1\n            if window_label == 'begin':\n                # Repeat the last row window times to generate a set of the same size as the input\n                index[[-1]] = windows[0] + 1\n\n            df_feats = df_feats.iloc[np.arange(len(df_feats)).repeat(index)].reset_index(drop = True)\n            \n            assert(len(df_feats) == len(tdcsfog))\n            \n            if test:\n                df_feats['Id'] = tdcsfog['Id']\n                df_feats = df_feats.merge(tdcsfog.drop(columns = [\"Time\"]), on = \"Id\")\n            else: \n                df_feats = df_feats.join(tdcsfog.drop(columns = [\"Time\"]))\n                \n            tdcsfog_tot = pd.concat([tdcsfog_tot,df_feats],ignore_index = True)\n            \n        defog_tot = pd.concat([tdcsfog_tot, defog_tot], ignore_index = True) \n               \n        return defog_tot\n","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:41:23.320444Z","iopub.execute_input":"2023-06-05T14:41:23.320814Z","iopub.status.idle":"2023-06-05T14:41:23.346466Z","shell.execute_reply.started":"2023-06-05T14:41:23.320775Z","shell.execute_reply":"2023-06-05T14:41:23.345001Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Collect the Defog and Tdcsfog train files","metadata":{}},{"cell_type":"code","source":"# Set the directory path to the folder containing the CSV files.\ntdcsfog_path = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog'\n\n# Initialize an empty list to store the dataframes.\ntdcsfog_list = []\n\n\n# Loop through each file in the directory and read it into a dataframe.\nfor file_name in os.listdir(tdcsfog_path):\n    if file_name.endswith('.csv'):\n        file_path = os.path.join(tdcsfog_path, file_name)\n        file = pd.read_csv(file_path)\n        file.Time = file.Time # / (len(file) - 1)\n        tdcsfog_list.append(file)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:41:23.347762Z","iopub.execute_input":"2023-06-05T14:41:23.348153Z","iopub.status.idle":"2023-06-05T14:41:37.574093Z","shell.execute_reply.started":"2023-06-05T14:41:23.348121Z","shell.execute_reply":"2023-06-05T14:41:37.573417Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Set the directory path to the folder containing the CSV files.\ndefog_path = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog'\n\n# Initialize an empty list to store the dataframes.\ndefog_list = []\n\n# Loop through each file in the directory and read it into a dataframe.\nfor file_name in os.listdir(defog_path):\n    if file_name.endswith('.csv'):\n        file_path = os.path.join(defog_path, file_name)\n        file = pd.read_csv(file_path)\n        file.Time = file.Time # / (len(file) - 1)\n        file = file[(file['Task'] == 1) & (file['Valid'] == 1)]\n        file = file.drop(columns = ['Task','Valid'])\n        defog_list.append(file)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:42:03.003230Z","iopub.execute_input":"2023-06-05T14:42:03.003580Z","iopub.status.idle":"2023-06-05T14:42:11.985760Z","shell.execute_reply.started":"2023-06-05T14:42:03.003557Z","shell.execute_reply":"2023-06-05T14:42:11.983793Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Set the training hyperparameters\n","metadata":{}},{"cell_type":"code","source":"# Input training data\ndf_list_train = [defog_list,tdcsfog_list]\n\n# Switch for train and test modes (mainly beacuse of ID's)\ntest = False","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:01:52.591876Z","iopub.execute_input":"2023-06-05T15:01:52.592230Z","iopub.status.idle":"2023-06-05T15:01:52.597806Z","shell.execute_reply.started":"2023-06-05T15:01:52.592203Z","shell.execute_reply":"2023-06-05T15:01:52.596456Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Extract features from the training data","metadata":{}},{"cell_type":"code","source":"train_features = extract_features(method = method, df_list = df_list_train, window_label = window_label, fc = fc_train, test = False)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:01:54.528620Z","iopub.execute_input":"2023-06-05T15:01:54.529014Z","iopub.status.idle":"2023-06-05T15:11:17.519722Z","shell.execute_reply.started":"2023-06-05T15:01:54.528983Z","shell.execute_reply":"2023-06-05T15:11:17.518589Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a5d65b2f59340249e9e12d9d780b92a"}},"metadata":{}}]},{"cell_type":"markdown","source":"It is better to reduce the memory usage. Reference: [Reducing DataFrame memory size by ~65%](https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65)","metadata":{"papermill":{"duration":0.027012,"end_time":"2023-05-14T06:28:57.01431","exception":false,"start_time":"2023-05-14T06:28:56.987298","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def reduce_memory_usage(df):\n    \n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype.name\n        if ((col_type != 'datetime64[ns]') & (col_type != 'category')):\n            if (col_type != 'object'):\n                c_min = df[col].min()\n                c_max = df[col].max()\n\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)\n\n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        pass\n            else:\n                df[col] = df[col].astype('category')\n    mem_usg = df.memory_usage().sum() / 1024 ** 2 \n    print(\"Memory usage became: \",mem_usg,\" MB\")\n    \n    return df","metadata":{"papermill":{"duration":0.050491,"end_time":"2023-05-14T06:28:57.092322","exception":false,"start_time":"2023-05-14T06:28:57.041831","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T15:11:33.908576Z","iopub.execute_input":"2023-06-05T15:11:33.908947Z","iopub.status.idle":"2023-06-05T15:11:33.922059Z","shell.execute_reply.started":"2023-06-05T15:11:33.908923Z","shell.execute_reply":"2023-06-05T15:11:33.919961Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# train_features = reduce_memory_usage(train_features)","metadata":{"papermill":{"duration":0.635449,"end_time":"2023-05-14T06:28:57.755209","exception":false,"start_time":"2023-05-14T06:28:57.11976","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-29T17:34:54.125908Z","iopub.execute_input":"2023-05-29T17:34:54.126539Z","iopub.status.idle":"2023-05-29T17:34:54.130730Z","shell.execute_reply.started":"2023-05-29T17:34:54.126507Z","shell.execute_reply":"2023-05-29T17:34:54.129615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features.describe()","metadata":{"papermill":{"duration":3.461726,"end_time":"2023-05-14T06:29:01.244355","exception":false,"start_time":"2023-05-14T06:28:57.782629","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T15:11:38.948978Z","iopub.execute_input":"2023-06-05T15:11:38.950092Z","iopub.status.idle":"2023-06-05T15:11:39.454197Z","shell.execute_reply.started":"2023-06-05T15:11:38.950050Z","shell.execute_reply":"2023-06-05T15:11:39.453153Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"       AccAP__abs_diff_mean__w=220  AccAP__amax__w=220  AccAP__amin__w=220  \\\ncount                441556.000000       441556.000000       441556.000000   \nmean                      0.115005            2.519651           -0.462276   \nstd                       0.151252            3.251904            2.232631   \nmin                       0.000906           -5.173580          -47.829639   \n25%                       0.013710           -0.014055           -0.809772   \n50%                       0.046373            1.815238           -0.340159   \n75%                       0.174506            4.935745            0.164579   \nmax                       1.876264           30.337694            4.646983   \n\n       AccAP__diff_std__w=220  AccAP__kurtosis__w=220  AccAP__mean__w=220  \\\ncount           441556.000000           441556.000000       441556.000000   \nmean                 0.199022                1.312212            0.842386   \nstd                  0.307837                4.970731            1.671038   \nmin                  0.001216               -1.998612           -5.234686   \n25%                  0.021620               -0.441706           -0.235479   \n50%                  0.084631                0.241656            0.170655   \n75%                  0.278594                1.530805            2.100238   \nmax                  6.794604              212.830006            6.654660   \n\n       AccAP__skew__w=220  AccAP__slope__w=220  AccAP__std__w=220  \\\ncount       441556.000000         4.415560e+05      441556.000000   \nmean             0.101392        -2.049173e+01           0.539932   \nstd              0.926183         1.348796e+04           0.654040   \nmin            -14.582901        -8.959771e+06           0.001775   \n25%             -0.374899        -4.930423e-01           0.064029   \n50%              0.074292        -4.577651e-02           0.304523   \n75%              0.575709         2.342964e-01           0.797185   \nmax             14.619858         1.089646e+05           6.842426   \n\n       AccAP__sum__w=220  ...  AccV__slope__w=220  AccV__std__w=220  \\\ncount      441556.000000  ...       441556.000000     441556.000000   \nmean          185.324896  ...            0.011351          0.559785   \nstd           367.628411  ...            0.164592          0.605888   \nmin         -1151.630820  ...           -3.774559          0.001906   \n25%           -51.805342  ...           -0.062936          0.073671   \n50%            37.544107  ...            0.000049          0.294529   \n75%           462.052342  ...            0.066994          0.938178   \nmax          1464.025223  ...           16.618226          4.264629   \n\n       AccV__sum__w=220  AccV__var__w=220           AccV          AccML  \\\ncount     441556.000000     441556.000000  441556.000000  441556.000000   \nmean       -1324.731133          0.680459      -6.021496       0.041497   \nstd          909.790311          1.251101       4.216882       0.921045   \nmin        -2252.253884          0.000004     -34.817838     -12.676490   \n25%        -2097.075851          0.005427      -9.509894      -0.242282   \n50%        -2009.676292          0.086747      -8.566946       0.058581   \n75%         -216.354846          0.880177      -0.983154       0.314582   \nmax         -156.753495         18.187059       3.476052      12.952856   \n\n               AccAP  StartHesitation           Turn        Walking  \ncount  441556.000000    441556.000000  441556.000000  441556.000000  \nmean        0.842386         0.000661       0.131016       0.007048  \nstd         1.873943         0.025707       0.337418       0.083655  \nmin       -47.829639         0.000000       0.000000       0.000000  \n25%        -0.253848         0.000000       0.000000       0.000000  \n50%         0.121350         0.000000       0.000000       0.000000  \n75%         2.153545         0.000000       0.000000       0.000000  \nmax        30.337694         1.000000       1.000000       1.000000  \n\n[8 rows x 39 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AccAP__abs_diff_mean__w=220</th>\n      <th>AccAP__amax__w=220</th>\n      <th>AccAP__amin__w=220</th>\n      <th>AccAP__diff_std__w=220</th>\n      <th>AccAP__kurtosis__w=220</th>\n      <th>AccAP__mean__w=220</th>\n      <th>AccAP__skew__w=220</th>\n      <th>AccAP__slope__w=220</th>\n      <th>AccAP__std__w=220</th>\n      <th>AccAP__sum__w=220</th>\n      <th>...</th>\n      <th>AccV__slope__w=220</th>\n      <th>AccV__std__w=220</th>\n      <th>AccV__sum__w=220</th>\n      <th>AccV__var__w=220</th>\n      <th>AccV</th>\n      <th>AccML</th>\n      <th>AccAP</th>\n      <th>StartHesitation</th>\n      <th>Turn</th>\n      <th>Walking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n      <td>4.415560e+05</td>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n      <td>...</td>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n      <td>441556.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.115005</td>\n      <td>2.519651</td>\n      <td>-0.462276</td>\n      <td>0.199022</td>\n      <td>1.312212</td>\n      <td>0.842386</td>\n      <td>0.101392</td>\n      <td>-2.049173e+01</td>\n      <td>0.539932</td>\n      <td>185.324896</td>\n      <td>...</td>\n      <td>0.011351</td>\n      <td>0.559785</td>\n      <td>-1324.731133</td>\n      <td>0.680459</td>\n      <td>-6.021496</td>\n      <td>0.041497</td>\n      <td>0.842386</td>\n      <td>0.000661</td>\n      <td>0.131016</td>\n      <td>0.007048</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.151252</td>\n      <td>3.251904</td>\n      <td>2.232631</td>\n      <td>0.307837</td>\n      <td>4.970731</td>\n      <td>1.671038</td>\n      <td>0.926183</td>\n      <td>1.348796e+04</td>\n      <td>0.654040</td>\n      <td>367.628411</td>\n      <td>...</td>\n      <td>0.164592</td>\n      <td>0.605888</td>\n      <td>909.790311</td>\n      <td>1.251101</td>\n      <td>4.216882</td>\n      <td>0.921045</td>\n      <td>1.873943</td>\n      <td>0.025707</td>\n      <td>0.337418</td>\n      <td>0.083655</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000906</td>\n      <td>-5.173580</td>\n      <td>-47.829639</td>\n      <td>0.001216</td>\n      <td>-1.998612</td>\n      <td>-5.234686</td>\n      <td>-14.582901</td>\n      <td>-8.959771e+06</td>\n      <td>0.001775</td>\n      <td>-1151.630820</td>\n      <td>...</td>\n      <td>-3.774559</td>\n      <td>0.001906</td>\n      <td>-2252.253884</td>\n      <td>0.000004</td>\n      <td>-34.817838</td>\n      <td>-12.676490</td>\n      <td>-47.829639</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.013710</td>\n      <td>-0.014055</td>\n      <td>-0.809772</td>\n      <td>0.021620</td>\n      <td>-0.441706</td>\n      <td>-0.235479</td>\n      <td>-0.374899</td>\n      <td>-4.930423e-01</td>\n      <td>0.064029</td>\n      <td>-51.805342</td>\n      <td>...</td>\n      <td>-0.062936</td>\n      <td>0.073671</td>\n      <td>-2097.075851</td>\n      <td>0.005427</td>\n      <td>-9.509894</td>\n      <td>-0.242282</td>\n      <td>-0.253848</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.046373</td>\n      <td>1.815238</td>\n      <td>-0.340159</td>\n      <td>0.084631</td>\n      <td>0.241656</td>\n      <td>0.170655</td>\n      <td>0.074292</td>\n      <td>-4.577651e-02</td>\n      <td>0.304523</td>\n      <td>37.544107</td>\n      <td>...</td>\n      <td>0.000049</td>\n      <td>0.294529</td>\n      <td>-2009.676292</td>\n      <td>0.086747</td>\n      <td>-8.566946</td>\n      <td>0.058581</td>\n      <td>0.121350</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.174506</td>\n      <td>4.935745</td>\n      <td>0.164579</td>\n      <td>0.278594</td>\n      <td>1.530805</td>\n      <td>2.100238</td>\n      <td>0.575709</td>\n      <td>2.342964e-01</td>\n      <td>0.797185</td>\n      <td>462.052342</td>\n      <td>...</td>\n      <td>0.066994</td>\n      <td>0.938178</td>\n      <td>-216.354846</td>\n      <td>0.880177</td>\n      <td>-0.983154</td>\n      <td>0.314582</td>\n      <td>2.153545</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.876264</td>\n      <td>30.337694</td>\n      <td>4.646983</td>\n      <td>6.794604</td>\n      <td>212.830006</td>\n      <td>6.654660</td>\n      <td>14.619858</td>\n      <td>1.089646e+05</td>\n      <td>6.842426</td>\n      <td>1464.025223</td>\n      <td>...</td>\n      <td>16.618226</td>\n      <td>4.264629</td>\n      <td>-156.753495</td>\n      <td>18.187059</td>\n      <td>3.476052</td>\n      <td>12.952856</td>\n      <td>30.337694</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 39 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Create Dataset\n\nFirst, we need to **split the data into input features (i.e. \"Time\", \"AccV\", \"AccML\", and \"AccAP\") and target variables (i.e. \"StartHesitation\", \"Turn\", and \"Walking\")**. We can do this using the .iloc method to select the appropriate columns.","metadata":{"papermill":{"duration":0.036051,"end_time":"2023-05-14T06:34:24.891554","exception":false,"start_time":"2023-05-14T06:34:24.855503","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Use smote to create synthetic data\n\n# smote = SMOTE(random_state = 4, k_neighbors=100)\n# X_syn, y_syn = smote.fit_resample(X_merged, merged['label'])\n\n\n# Create Synthetic dataset\n# syn = pd.concat([X_syn,y_syn.to_frame(name = \"label\")], axis=1)\n# syn[\"Turn\"], syn[\"Walking\"], syn[\"StartHesitation\"] = (syn[\"label\"] == 1).astype(int), (syn[\"label\"] == 2).astype(int), (syn[\"label\"] == 3).astype(int)\n\n# tot = pd.concat([merged,syn])\n# tot = tot.sort_values(\"Time\",ignore_index = True)\n\n# Normalize time\n# tot[\"Time\"] = tot[\"Time\"] / (len(tot) - 1)","metadata":{"papermill":{"duration":0.208778,"end_time":"2023-05-14T06:34:25.136244","exception":false,"start_time":"2023-05-14T06:34:24.927466","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-25T20:14:32.544673Z","iopub.execute_input":"2023-05-25T20:14:32.545685Z","iopub.status.idle":"2023-05-25T20:14:32.551376Z","shell.execute_reply.started":"2023-05-25T20:14:32.545643Z","shell.execute_reply":"2023-05-25T20:14:32.550340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data = np.array([train_features['Walking__mean__w='+ str(windows[0])], train_features['StartHesitation__mean__w=' + str(windows[0])], train_features['Turn__mean__w='+ str(windows[0])]])\n\n# labels = np.argmax(data, axis = 0)\n# sums = np.sum(data, axis = 0)\n# labels = np.where(sums == 0 , 5, labels)\n\ny1 = train_features['StartHesitation']  # target variable for StartHesitation\ny2 = train_features['Turn']  # target variable for Turn\ny3 = train_features['Walking']  # target variable for Walking\n\ntrain_features = train_features.drop(columns = [\"StartHesitation\",\"Turn\",\"Walking\"])\n\n# Change this by hand if you want to try more features\nX_tot = pd.concat([train_features.iloc[:, :(len(funcs) * len(series_names))],train_features.iloc[:, -len(series_names):]], axis = 1, ignore_index = False)\nX_tot = X_tot.fillna(0.0)\nprint(X_tot.columns)\n\n# train_features['Walking'] = np.where(labels == 0 , 1, 0)\n# train_features['StartHesitation'] = np.where(labels == 1 , 1, 0)\n# train_features['Turn'] = np.where(labels == 2 , 1, 0)\n\n\n# y1 = train_features['StartHesitation']  # target variable for StartHesitation\n# y2 = train_features['Turn']  # target variable for Turn\n# y3 = train_features['Walking']  # target variable for Walking\n","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:11:45.544562Z","iopub.execute_input":"2023-06-05T15:11:45.545018Z","iopub.status.idle":"2023-06-05T15:11:45.685949Z","shell.execute_reply.started":"2023-06-05T15:11:45.544987Z","shell.execute_reply":"2023-06-05T15:11:45.685108Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Index(['AccAP__abs_diff_mean__w=220', 'AccAP__amax__w=220',\n       'AccAP__amin__w=220', 'AccAP__diff_std__w=220',\n       'AccAP__kurtosis__w=220', 'AccAP__mean__w=220', 'AccAP__skew__w=220',\n       'AccAP__slope__w=220', 'AccAP__std__w=220', 'AccAP__sum__w=220',\n       'AccAP__var__w=220', 'AccML__abs_diff_mean__w=220',\n       'AccML__amax__w=220', 'AccML__amin__w=220', 'AccML__diff_std__w=220',\n       'AccML__kurtosis__w=220', 'AccML__mean__w=220', 'AccML__skew__w=220',\n       'AccML__slope__w=220', 'AccML__std__w=220', 'AccML__sum__w=220',\n       'AccML__var__w=220', 'AccV__abs_diff_mean__w=220', 'AccV__amax__w=220',\n       'AccV__amin__w=220', 'AccV__diff_std__w=220', 'AccV__kurtosis__w=220',\n       'AccV__mean__w=220', 'AccV__skew__w=220', 'AccV__slope__w=220',\n       'AccV__std__w=220', 'AccV__sum__w=220', 'AccV__var__w=220', 'AccV',\n       'AccML', 'AccAP'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Most of the target variables are 0. We had better **create each balanced dataset with the target variables of 0 and 1 equally**.","metadata":{"papermill":{"duration":0.035961,"end_time":"2023-05-14T06:34:25.208958","exception":false,"start_time":"2023-05-14T06:34:25.172997","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Find the positions of y1 where it equals 0.\ny1_zeros = np.where(y1 == 0)[0]\ny1_ones = np.where(y1 == 1)[0]\n\n# Choose the same number of samples with y1 == 1 as there are with y1 == 0.\nnum1_ones = (y1 == 1).sum()\nnp.random.seed(42)\ny1_zeros = np.random.choice(np.where(y1 == 0)[0], size = num1_ones, replace = False)\n\n# Combine the positions of y1 == 0 and y1 == 1.\ny1_balanced_idxs = np.sort(np.concatenate([y1_zeros, y1_ones]))\n\n# Use the balanced indices to get the corresponding rows of X and y1.\nX1_balanced = X_tot.iloc[y1_balanced_idxs, :]\ny1_balanced = y1.iloc[y1_balanced_idxs]","metadata":{"papermill":{"duration":0.760961,"end_time":"2023-05-14T06:34:26.007999","exception":false,"start_time":"2023-05-14T06:34:25.247038","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T15:11:50.647863Z","iopub.execute_input":"2023-06-05T15:11:50.649006Z","iopub.status.idle":"2023-06-05T15:11:50.673931Z","shell.execute_reply.started":"2023-06-05T15:11:50.648964Z","shell.execute_reply":"2023-06-05T15:11:50.672384Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Find the positions of y2 where it equals 0.\ny2_zeros = np.where(y2 == 0)[0]\ny2_ones = np.where(y2 == 1)[0]\n\n# Choose the same number of samples with y2 == 1 as there are with y2 == 0.\nnum2_ones = (y2 == 1).sum()\nnp.random.seed(42)\ny2_zeros = np.random.choice(np.where(y2 == 0)[0], size = num2_ones, replace = False)\n\n# Combine the positions of y2 == 0 and y2 == 1.\ny2_balanced_idxs = np.sort(np.concatenate([y2_zeros, y2_ones]))\n\n# Use the balanced indices to get the corresponding rows of X and y1.\nX2_balanced = X_tot.iloc[y2_balanced_idxs, :]\ny2_balanced = y2.iloc[y2_balanced_idxs]","metadata":{"papermill":{"duration":1.218739,"end_time":"2023-05-14T06:34:27.263905","exception":false,"start_time":"2023-05-14T06:34:26.045166","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T15:11:53.378800Z","iopub.execute_input":"2023-06-05T15:11:53.379604Z","iopub.status.idle":"2023-06-05T15:11:53.423450Z","shell.execute_reply.started":"2023-06-05T15:11:53.379563Z","shell.execute_reply":"2023-06-05T15:11:53.422296Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Find the positions of y3 where it equals 0.\ny3_zeros = np.where(y3 == 0)[0]\ny3_ones = np.where(y3 == 1)[0]\n\n# Choose the same number of samples with y3 == 1 as there are with y3 == 0.\nnum3_ones = (y3 == 1).sum()\nnp.random.seed(42)\ny3_zeros = np.random.choice(np.where(y3 == 0)[0], size = num3_ones, replace = False)\n\n# Combine the positions of y3 == 0 and y3 == 1.\ny3_balanced_idxs = np.sort(np.concatenate([y3_zeros, y3_ones]))\n\n# Use the balanced indices to get the corresponding rows of X and y3.\nX3_balanced = X_tot.iloc[y3_balanced_idxs, :]\ny3_balanced = y3.iloc[y3_balanced_idxs]","metadata":{"papermill":{"duration":0.801883,"end_time":"2023-05-14T06:34:28.102399","exception":false,"start_time":"2023-05-14T06:34:27.300516","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T15:11:56.091025Z","iopub.execute_input":"2023-06-05T15:11:56.091679Z","iopub.status.idle":"2023-06-05T15:11:56.120510Z","shell.execute_reply.started":"2023-06-05T15:11:56.091646Z","shell.execute_reply":"2023-06-05T15:11:56.119499Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"Next, we can **split the data into training and testing sets using the train_test_split function from scikit-learn**.","metadata":{"papermill":{"duration":0.036394,"end_time":"2023-05-14T06:34:28.17924","exception":false,"start_time":"2023-05-14T06:34:28.142846","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1_balanced, y1_balanced, test_size = 0.2, random_state = 42)\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2_balanced, y2_balanced, test_size = 0.2, random_state = 42)\nX3_train, X3_test, y3_train, y3_test = train_test_split(X3_balanced, y3_balanced, test_size = 0.2, random_state = 42)","metadata":{"papermill":{"duration":1.062844,"end_time":"2023-05-14T06:34:29.279157","exception":false,"start_time":"2023-05-14T06:34:28.216313","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T15:11:58.401621Z","iopub.execute_input":"2023-06-05T15:11:58.402187Z","iopub.status.idle":"2023-06-05T15:11:58.447492Z","shell.execute_reply.started":"2023-06-05T15:11:58.402157Z","shell.execute_reply":"2023-06-05T15:11:58.446394Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"Then, we **standardize the independent variables**.","metadata":{"papermill":{"duration":0.035687,"end_time":"2023-05-14T06:34:29.350635","exception":false,"start_time":"2023-05-14T06:34:29.314948","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Standardize the independent variables.\nscaler1 = StandardScaler()\nX1_train = scaler1.fit_transform(X1_train)\nX1_test = scaler1.transform(X1_test)\n\nscaler2 = StandardScaler()\nX2_train = scaler2.fit_transform(X2_train)\nX2_test = scaler2.transform(X2_test)\n\nscaler3 = StandardScaler()\nX3_train = scaler3.fit_transform(X3_train)\nX3_test = scaler3.transform(X3_test)","metadata":{"papermill":{"duration":0.910372,"end_time":"2023-05-14T06:34:30.297294","exception":false,"start_time":"2023-05-14T06:34:29.386922","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T15:12:01.550278Z","iopub.execute_input":"2023-06-05T15:12:01.550942Z","iopub.status.idle":"2023-06-05T15:12:01.604350Z","shell.execute_reply.started":"2023-06-05T15:12:01.550908Z","shell.execute_reply":"2023-06-05T15:12:01.603647Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Create, Train, and Evaluate Model\n\nFinally, we can **create and train three separate models**, one for each target variable, using a suitable algorithm.\n       \n### This time we use **Random Forest Regressor instead of the Logistic Regression model**.\n\n**For a Logistic Regression model, please see [PD FOG Prediction Baseline by Logistic Regression](https://www.kaggle.com/code/gokifujiya/pd-fog-prediction-baseline-by-logistic-regression).**","metadata":{"papermill":{"duration":0.035684,"end_time":"2023-05-14T06:34:30.369288","exception":false,"start_time":"2023-05-14T06:34:30.333604","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n# from autosklearn.classification import AutoSklearnClassifier\n\n# Create three separate logistic regression models.\n#model1 = LogisticRegression()\n#model2 = LogisticRegression()\n#model3 = LogisticRegression()\n\n# Define the parameter grid for hyperparameter tuning.\n# param_grid = {\n#     # 'n_estimators': [100, 200, 300],\n#     'n_estimators': [200],\n#     'max_depth': [8],\n#     'n_jobs': [-1],\n#     'random_state': [42]\n# }\n\nmodel1 = AdaBoostRegressor(n_estimators = 200, random_state = 42)\nmodel2 = AdaBoostRegressor(n_estimators = 200, random_state = 42)\nmodel3 = AdaBoostRegressor(n_estimators = 200, random_state = 42)\n# model1 = AutoSklearnClassifier()\n# model2 = AutoSklearnClassifier()\n# model3 = AutoSklearnClassifier()\n\n\n# Create three separate Random Forest Regressor models.\n# model1 = GridSearchCV(RandomForestRegressor(), param_grid, cv = 5)\n# model2 = GridSearchCV(RandomForestRegressor(), param_grid, cv = 5)\n# model3 = GridSearchCV(RandomForestRegressor(), param_grid, cv = 5)\n\n# Train the models on the training data.\nmodel1.fit(X1_train, y1_train)\nmodel2.fit(X2_train, y2_train)\nmodel3.fit(X3_train, y3_train)\n\n# Evaluate the models on the test data.\nprint('R2 for StartHesitation:', model1.score(X1_test, y1_test))\nprint('R2 for Turn:', model2.score(X2_test, y2_test))\nprint('R2 for Walking:', model3.score(X3_test, y3_test))\n\n# # Evaluate the models on the test data.\n# print('R2 for StartHesitation:', model1.best_score_)\n# print('R2 for Turn:', model2.best_score_)\n# print('R2 for Walking:', model3.best_score_)\n\n# # Print the best parameters for each model\n# print('Best parameters for StartHesitation:', model1.best_params_)\n# print('Best parameters for Turn:', model2.best_params_)\n# print('Best parameters for Walking:', model3.best_params_)","metadata":{"papermill":{"duration":390.611171,"end_time":"2023-05-14T06:41:01.016047","exception":false,"start_time":"2023-05-14T06:34:30.404876","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T15:12:05.112890Z","iopub.execute_input":"2023-06-05T15:12:05.113241Z","iopub.status.idle":"2023-06-05T15:12:12.478762Z","shell.execute_reply.started":"2023-06-05T15:12:05.113214Z","shell.execute_reply":"2023-06-05T15:12:12.477875Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"R2 for StartHesitation: 0.9657894736842105\nR2 for Turn: 0.3059505533321044\nR2 for Walking: 0.7065706384151265\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Create Test Dataset","metadata":{"papermill":{"duration":0.036298,"end_time":"2023-05-14T06:47:51.363758","exception":false,"start_time":"2023-05-14T06:47:51.32746","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Set the directory path to the folder containing the CSV files.\ntdcsfog_test_path = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/tdcsfog'\n\n# Initialize an empty list to store the dataframes.\ntdcsfog_test_list = []\n\n# Loop through each file in the directory and read it into a dataframe.\nfor file_name in os.listdir(tdcsfog_test_path):\n    if file_name.endswith('.csv'):\n        file_path = os.path.join(tdcsfog_test_path, file_name)\n        file = pd.read_csv(file_path)\n        file['Id'] = file_name[:-4] + '_' + file['Time'].apply(str)\n        file.Time = file.Time \n        tdcsfog_test_list.append(file)","metadata":{"papermill":{"duration":0.1014,"end_time":"2023-05-14T06:47:51.501918","exception":false,"start_time":"2023-05-14T06:47:51.400518","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T15:13:31.114263Z","iopub.execute_input":"2023-06-05T15:13:31.114604Z","iopub.status.idle":"2023-06-05T15:13:31.140491Z","shell.execute_reply.started":"2023-06-05T15:13:31.114581Z","shell.execute_reply":"2023-06-05T15:13:31.139222Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Set the directory path to the folder containing the CSV files.\ndefog_test_path = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/defog'\n\n# Initialize an empty list to store the dataframes.\ndefog_test_list = []\n\n# Loop through each file in the directory and read it into a dataframe.\nfor file_name in os.listdir(defog_test_path):\n    if file_name.endswith('.csv'):\n        file_path = os.path.join(defog_test_path, file_name)\n        file = pd.read_csv(file_path)\n        file['Id'] = file_name[:-4] + '_' + file['Time'].apply(str)\n        file.Time = file.Time\n        defog_test_list.append(file)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:13:33.364863Z","iopub.execute_input":"2023-06-05T15:13:33.365473Z","iopub.status.idle":"2023-06-05T15:13:33.731214Z","shell.execute_reply.started":"2023-06-05T15:13:33.365442Z","shell.execute_reply":"2023-06-05T15:13:33.729569Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def slope(x): return (x[-1] - x[0]) / x[0] if x[0] else 0\ndef abs_diff_mean(x): return np.mean(np.abs(x[1:] - x[:-1])) if len(x) > 1 else 0\ndef diff_std(x): return np.std(x[1:] - x[:-1]) if len(x) > 1 else 0\n\n\n\nfuncs = [make_robust(f) for f in [np.min,np.var, np.max, np.std, np.mean, slope, ss.skew, ss.kurtosis, abs_diff_mean, diff_std, np.sum,]]\n\nfc_test = FeatureCollection(\n    MultipleFeatureDescriptors(\n          functions=funcs,\n          series_names=[\"AccV\", \"AccML\", \"AccAP\"],\n          windows=windows,\n          strides=strides[0],\n    )\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:13:36.560640Z","iopub.execute_input":"2023-06-05T15:13:36.561039Z","iopub.status.idle":"2023-06-05T15:13:36.571538Z","shell.execute_reply.started":"2023-06-05T15:13:36.561010Z","shell.execute_reply":"2023-06-05T15:13:36.570074Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Set the testing hyperparameters","metadata":{}},{"cell_type":"code","source":"# Input testing data\ndf_list_test = [defog_test_list,tdcsfog_test_list]\n\n# Switch for train and test modes (mainly beacuse of ID's)\ntest = True","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:13:39.306665Z","iopub.execute_input":"2023-06-05T15:13:39.307052Z","iopub.status.idle":"2023-06-05T15:13:39.312808Z","shell.execute_reply.started":"2023-06-05T15:13:39.307027Z","shell.execute_reply":"2023-06-05T15:13:39.311458Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"test_features = extract_features(method = method, df_list = df_list_test, window_label = window_label, fc = fc_test, test = test)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:13:57.356656Z","iopub.execute_input":"2023-06-05T15:13:57.357028Z","iopub.status.idle":"2023-06-05T15:19:57.413459Z","shell.execute_reply.started":"2023-06-05T15:13:57.357001Z","shell.execute_reply":"2023-06-05T15:19:57.412257Z"},"trusted":true},"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1d52d08747e4ca48bc84214000a9a72"}},"metadata":{}}]},{"cell_type":"code","source":"# test_features = reduce_memory_usage(test_features)","metadata":{"papermill":{"duration":0.686019,"end_time":"2023-05-14T06:47:53.049474","exception":false,"start_time":"2023-05-14T06:47:52.363455","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-28T19:15:56.259219Z","iopub.execute_input":"2023-05-28T19:15:56.259930Z","iopub.status.idle":"2023-05-28T19:15:56.269276Z","shell.execute_reply.started":"2023-05-28T19:15:56.259860Z","shell.execute_reply":"2023-05-28T19:15:56.267937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_features","metadata":{"papermill":{"duration":0.234153,"end_time":"2023-05-14T06:47:53.320407","exception":false,"start_time":"2023-05-14T06:47:53.086254","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-28T19:15:59.029121Z","iopub.execute_input":"2023-05-28T19:15:59.029961Z","iopub.status.idle":"2023-05-28T19:15:59.194872Z","shell.execute_reply.started":"2023-05-28T19:15:59.029914Z","shell.execute_reply":"2023-05-28T19:15:59.193443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{"papermill":{"duration":0.037342,"end_time":"2023-05-14T06:47:53.395359","exception":false,"start_time":"2023-05-14T06:47:53.358017","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Separate the dataset for the independent variables.\n# Change by hand\ntest_X = pd.concat([test_features.iloc[:, :(len(funcs) * len(series_names))],test_features.iloc[:, -len(series_names):]], axis = 1, ignore_index = False)\n\nprint(test_X)\n# Standardize the independent variables by a new scaler.\nscaler = StandardScaler()\ntest_X = scaler.fit_transform(test_X)\n\n# Get the predictions for the three models on the test data.\npred_y1 = model1.predict(test_X)\npred_y2 = model2.predict(test_X)\npred_y3 = model3.predict(test_X)\n\ntest_features['StartHesitation'] = pred_y1 # target variable for StartHesitation\ntest_features['Turn'] = pred_y2 # target variable for Turn\ntest_features['Walking'] = pred_y3 # target variable for Walking\n\nprint(test_features)","metadata":{"papermill":{"duration":1.047587,"end_time":"2023-05-14T06:47:54.480581","exception":false,"start_time":"2023-05-14T06:47:53.432994","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T15:31:57.591281Z","iopub.execute_input":"2023-06-05T15:31:57.591711Z","iopub.status.idle":"2023-06-05T15:31:58.177718Z","shell.execute_reply.started":"2023-06-05T15:31:57.591676Z","shell.execute_reply":"2023-06-05T15:31:58.175583Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"        AccAP__abs_diff_mean__w=220  AccAP__amax__w=220  AccAP__amin__w=220  \\\n0                          0.007664           -1.368046           -1.569344   \n1                          0.007664           -1.368046           -1.569344   \n2                          0.007664           -1.368046           -1.569344   \n3                          0.007664           -1.368046           -1.569344   \n4                          0.007664           -1.368046           -1.569344   \n...                             ...                 ...                 ...   \n286365                     0.001221            0.398115            0.387910   \n286366                     0.001221            0.398115            0.387910   \n286367                     0.001221            0.398115            0.387910   \n286368                     0.001221            0.398115            0.387910   \n286369                     0.001221            0.398115            0.387910   \n\n        AccAP__diff_std__w=220  AccAP__kurtosis__w=220  AccAP__mean__w=220  \\\n0                     0.009896               -1.402810           -1.462197   \n1                     0.009896               -1.402810           -1.462197   \n2                     0.009896               -1.402810           -1.462197   \n3                     0.009896               -1.402810           -1.462197   \n4                     0.009896               -1.402810           -1.462197   \n...                        ...                     ...                 ...   \n286365                0.001555               -0.145132            0.393532   \n286366                0.001555               -0.145132            0.393532   \n286367                0.001555               -0.145132            0.393532   \n286368                0.001555               -0.145132            0.393532   \n286369                0.001555               -0.145132            0.393532   \n\n        AccAP__skew__w=220  AccAP__slope__w=220  AccAP__std__w=220  \\\n0                -0.299618             0.100756           0.057128   \n1                -0.299618             0.100756           0.057128   \n2                -0.299618             0.100756           0.057128   \n3                -0.299618             0.100756           0.057128   \n4                -0.299618             0.100756           0.057128   \n...                    ...                  ...                ...   \n286365           -0.380052            -0.000578           0.001975   \n286366           -0.380052            -0.000578           0.001975   \n286367           -0.380052            -0.000578           0.001975   \n286368           -0.380052            -0.000578           0.001975   \n286369           -0.380052            -0.000578           0.001975   \n\n        AccAP__sum__w=220  ...  AccV__kurtosis__w=220  AccV__mean__w=220  \\\n0             -321.683362  ...              -0.615461          -9.525887   \n1             -321.683362  ...              -0.615461          -9.525887   \n2             -321.683362  ...              -0.615461          -9.525887   \n3             -321.683362  ...              -0.615461          -9.525887   \n4             -321.683362  ...              -0.615461          -9.525887   \n...                   ...  ...                    ...                ...   \n286365          86.577022  ...              -0.021394          -0.898681   \n286366          86.577022  ...              -0.021394          -0.898681   \n286367          86.577022  ...              -0.021394          -0.898681   \n286368          86.577022  ...              -0.021394          -0.898681   \n286369          86.577022  ...              -0.021394          -0.898681   \n\n        AccV__skew__w=220  AccV__slope__w=220  AccV__std__w=220  \\\n0                0.520871           -0.001896          0.010096   \n1                0.520871           -0.001896          0.010096   \n2                0.520871           -0.001896          0.010096   \n3                0.520871           -0.001896          0.010096   \n4                0.520871           -0.001896          0.010096   \n...                   ...                 ...               ...   \n286365          -0.297066            0.000452          0.001049   \n286366          -0.297066            0.000452          0.001049   \n286367          -0.297066            0.000452          0.001049   \n286368          -0.297066            0.000452          0.001049   \n286369          -0.297066            0.000452          0.001049   \n\n        AccV__sum__w=220  AccV__var__w=220      AccV     AccML     AccAP  \n0           -2095.695094          0.000102 -9.533939  0.566322 -1.413525  \n1           -2095.695094          0.000102 -9.536140  0.564137 -1.440621  \n2           -2095.695094          0.000102 -9.529345  0.561765 -1.429332  \n3           -2095.695094          0.000102 -9.531239  0.564227 -1.415490  \n4           -2095.695094          0.000102 -9.540825  0.561854 -1.429471  \n...                  ...               ...       ...       ...       ...  \n286365       -197.709776          0.000001 -0.899299 -0.232668  0.389147  \n286366       -197.709776          0.000001 -0.901973 -0.233051  0.390114  \n286367       -197.709776          0.000001 -0.901690 -0.231888  0.391896  \n286368       -197.709776          0.000001 -0.899576 -0.232429  0.391106  \n286369       -197.709776          0.000001 -0.900289 -0.232363  0.390735  \n\n[286370 rows x 36 columns]\n        AccAP__abs_diff_mean__w=220  AccAP__amax__w=220  AccAP__amin__w=220  \\\n0                          0.007664           -1.368046           -1.569344   \n1                          0.007664           -1.368046           -1.569344   \n2                          0.007664           -1.368046           -1.569344   \n3                          0.007664           -1.368046           -1.569344   \n4                          0.007664           -1.368046           -1.569344   \n...                             ...                 ...                 ...   \n286365                     0.001221            0.398115            0.387910   \n286366                     0.001221            0.398115            0.387910   \n286367                     0.001221            0.398115            0.387910   \n286368                     0.001221            0.398115            0.387910   \n286369                     0.001221            0.398115            0.387910   \n\n        AccAP__diff_std__w=220  AccAP__kurtosis__w=220  AccAP__mean__w=220  \\\n0                     0.009896               -1.402810           -1.462197   \n1                     0.009896               -1.402810           -1.462197   \n2                     0.009896               -1.402810           -1.462197   \n3                     0.009896               -1.402810           -1.462197   \n4                     0.009896               -1.402810           -1.462197   \n...                        ...                     ...                 ...   \n286365                0.001555               -0.145132            0.393532   \n286366                0.001555               -0.145132            0.393532   \n286367                0.001555               -0.145132            0.393532   \n286368                0.001555               -0.145132            0.393532   \n286369                0.001555               -0.145132            0.393532   \n\n        AccAP__skew__w=220  AccAP__slope__w=220  AccAP__std__w=220  \\\n0                -0.299618             0.100756           0.057128   \n1                -0.299618             0.100756           0.057128   \n2                -0.299618             0.100756           0.057128   \n3                -0.299618             0.100756           0.057128   \n4                -0.299618             0.100756           0.057128   \n...                    ...                  ...                ...   \n286365           -0.380052            -0.000578           0.001975   \n286366           -0.380052            -0.000578           0.001975   \n286367           -0.380052            -0.000578           0.001975   \n286368           -0.380052            -0.000578           0.001975   \n286369           -0.380052            -0.000578           0.001975   \n\n        AccAP__sum__w=220  ...  AccV__std__w=220  AccV__sum__w=220  \\\n0             -321.683362  ...          0.010096      -2095.695094   \n1             -321.683362  ...          0.010096      -2095.695094   \n2             -321.683362  ...          0.010096      -2095.695094   \n3             -321.683362  ...          0.010096      -2095.695094   \n4             -321.683362  ...          0.010096      -2095.695094   \n...                   ...  ...               ...               ...   \n286365          86.577022  ...          0.001049       -197.709776   \n286366          86.577022  ...          0.001049       -197.709776   \n286367          86.577022  ...          0.001049       -197.709776   \n286368          86.577022  ...          0.001049       -197.709776   \n286369          86.577022  ...          0.001049       -197.709776   \n\n        AccV__var__w=220                 Id      AccV     AccML     AccAP  \\\n0               0.000102       003f117e14_0 -9.533939  0.566322 -1.413525   \n1               0.000102       003f117e14_1 -9.536140  0.564137 -1.440621   \n2               0.000102       003f117e14_2 -9.529345  0.561765 -1.429332   \n3               0.000102       003f117e14_3 -9.531239  0.564227 -1.415490   \n4               0.000102       003f117e14_4 -9.540825  0.561854 -1.429471   \n...                  ...                ...       ...       ...       ...   \n286365          0.000001  02ab235146_281683 -0.899299 -0.232668  0.389147   \n286366          0.000001  02ab235146_281684 -0.901973 -0.233051  0.390114   \n286367          0.000001  02ab235146_281685 -0.901690 -0.231888  0.391896   \n286368          0.000001  02ab235146_281686 -0.899576 -0.232429  0.391106   \n286369          0.000001  02ab235146_281687 -0.900289 -0.232363  0.390735   \n\n        StartHesitation      Turn   Walking  \n0                   0.0  0.066443  0.118138  \n1                   0.0  0.066443  0.118138  \n2                   0.0  0.066443  0.118138  \n3                   0.0  0.066443  0.118138  \n4                   0.0  0.066443  0.118138  \n...                 ...       ...       ...  \n286365              0.0  0.477411  0.009482  \n286366              0.0  0.477411  0.009482  \n286367              0.0  0.477411  0.009482  \n286368              0.0  0.477411  0.009482  \n286369              0.0  0.477411  0.009482  \n\n[286370 rows x 40 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Submission","metadata":{"papermill":{"duration":0.037147,"end_time":"2023-05-14T06:47:54.556874","exception":false,"start_time":"2023-05-14T06:47:54.519727","status":"completed"},"tags":[]}},{"cell_type":"code","source":"submission = test_features.loc[:,['Id','StartHesitation','Turn','Walking']].fillna(0.0)\nsubmission","metadata":{"papermill":{"duration":0.220266,"end_time":"2023-05-14T06:47:54.817263","exception":false,"start_time":"2023-05-14T06:47:54.596997","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T15:32:02.988652Z","iopub.execute_input":"2023-06-05T15:32:02.989061Z","iopub.status.idle":"2023-06-05T15:32:03.059728Z","shell.execute_reply.started":"2023-06-05T15:32:02.989031Z","shell.execute_reply":"2023-06-05T15:32:03.057748Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"                       Id  StartHesitation      Turn   Walking\n0            003f117e14_0              0.0  0.066443  0.118138\n1            003f117e14_1              0.0  0.066443  0.118138\n2            003f117e14_2              0.0  0.066443  0.118138\n3            003f117e14_3              0.0  0.066443  0.118138\n4            003f117e14_4              0.0  0.066443  0.118138\n...                   ...              ...       ...       ...\n286365  02ab235146_281683              0.0  0.477411  0.009482\n286366  02ab235146_281684              0.0  0.477411  0.009482\n286367  02ab235146_281685              0.0  0.477411  0.009482\n286368  02ab235146_281686              0.0  0.477411  0.009482\n286369  02ab235146_281687              0.0  0.477411  0.009482\n\n[286370 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>StartHesitation</th>\n      <th>Turn</th>\n      <th>Walking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>003f117e14_0</td>\n      <td>0.0</td>\n      <td>0.066443</td>\n      <td>0.118138</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>003f117e14_1</td>\n      <td>0.0</td>\n      <td>0.066443</td>\n      <td>0.118138</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>003f117e14_2</td>\n      <td>0.0</td>\n      <td>0.066443</td>\n      <td>0.118138</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003f117e14_3</td>\n      <td>0.0</td>\n      <td>0.066443</td>\n      <td>0.118138</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>003f117e14_4</td>\n      <td>0.0</td>\n      <td>0.066443</td>\n      <td>0.118138</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>286365</th>\n      <td>02ab235146_281683</td>\n      <td>0.0</td>\n      <td>0.477411</td>\n      <td>0.009482</td>\n    </tr>\n    <tr>\n      <th>286366</th>\n      <td>02ab235146_281684</td>\n      <td>0.0</td>\n      <td>0.477411</td>\n      <td>0.009482</td>\n    </tr>\n    <tr>\n      <th>286367</th>\n      <td>02ab235146_281685</td>\n      <td>0.0</td>\n      <td>0.477411</td>\n      <td>0.009482</td>\n    </tr>\n    <tr>\n      <th>286368</th>\n      <td>02ab235146_281686</td>\n      <td>0.0</td>\n      <td>0.477411</td>\n      <td>0.009482</td>\n    </tr>\n    <tr>\n      <th>286369</th>\n      <td>02ab235146_281687</td>\n      <td>0.0</td>\n      <td>0.477411</td>\n      <td>0.009482</td>\n    </tr>\n  </tbody>\n</table>\n<p>286370 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","metadata":{"papermill":{"duration":2.256699,"end_time":"2023-05-14T06:47:57.112488","exception":false,"start_time":"2023-05-14T06:47:54.855789","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-14T07:40:39.791585Z","iopub.execute_input":"2023-05-14T07:40:39.7924Z","iopub.status.idle":"2023-05-14T07:40:42.286661Z","shell.execute_reply.started":"2023-05-14T07:40:39.792362Z","shell.execute_reply":"2023-05-14T07:40:42.285704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save, Load, and Use Model\n\nTo save the trained Logistic Regression model, you can use the joblib library from the sklearn.externals module. This will save the model to a file in the current working directory. **To load the saved model later**, we can use the joblib.load() function.","metadata":{"papermill":{"duration":0.038824,"end_time":"2023-05-14T06:47:57.191566","exception":false,"start_time":"2023-05-14T06:47:57.152742","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import joblib\n\n# Save the model to disk.\njoblib.dump(model1, 'model1.joblib')\njoblib.dump(model2, 'model2.joblib')\njoblib.dump(model3, 'model3.joblib')\n\n# Load the saved models from disk.\nmodel1_loaded = joblib.load('model1.joblib')\nmodel2_loaded = joblib.load('model2.joblib')\nmodel3_loaded = joblib.load('model3.joblib')\n\n# Use the loaded models to make predictions on test data.\ny1_pred_loaded = model1_loaded.predict(test_X)\ny2_pred_loaded = model2_loaded.predict(test_X)\ny3_pred_loaded = model3_loaded.predict(test_X)","metadata":{"papermill":{"duration":1.274561,"end_time":"2023-05-14T06:47:58.505545","exception":false,"start_time":"2023-05-14T06:47:57.230984","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-14T07:40:42.289511Z","iopub.execute_input":"2023-05-14T07:40:42.290001Z","iopub.status.idle":"2023-05-14T07:40:43.569222Z","shell.execute_reply.started":"2023-05-14T07:40:42.289961Z","shell.execute_reply":"2023-05-14T07:40:43.567876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nIt is possible that **more features or more advanced machine learning algorithms** could improve the accuracy of the models. Additionally, it may be useful to **investigate other factors** that contribute to the occurrence of freezing of gait events, such as cognitive or environmental factors.","metadata":{"papermill":{"duration":0.037949,"end_time":"2023-05-14T06:47:58.581718","exception":false,"start_time":"2023-05-14T06:47:58.543769","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"I am a medical doctor working on **artificial intelligence (AI) for medicine**. At present AI is also widely used in the medical field. Particularly, AI performs in the healthcare sector following tasks: **image classification, object detection, semantic segmentation, GANs, text classification, etc**. **If you are interested in AI for medicine, please see my other notebooks.**","metadata":{"papermill":{"duration":0.037761,"end_time":"2023-05-14T06:47:58.657688","exception":false,"start_time":"2023-05-14T06:47:58.619927","status":"completed"},"tags":[]}}]}